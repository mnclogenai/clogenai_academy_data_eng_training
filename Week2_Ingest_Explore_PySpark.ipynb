{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# üìä Week 2 ‚Äî Ingest & Explore with PySpark\n",
    "### Building Reliable Data Pipelines in Databricks  \n",
    "\n",
    "---\n",
    "\n",
    "## **Goals for This Session**  \n",
    "By the end of this session, you will:\n",
    "\n",
    "- Understand how Spark ingests structured & semi-structured data  \n",
    "- Explore datasets using DataFrames  \n",
    "- Detect schema issues and data quality problems  \n",
    "- Compare formats: CSV, Parquet, and Delta  \n",
    "- Understand Delta features (ACID, schema evolution, time travel)  \n",
    "- See how this fits into real data pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Setup and Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **1. Data Ingestion**\n",
    "\n",
    "### Common Sources\n",
    "- Databricks FileStore / DBFS  \n",
    "- Local file uploads  \n",
    "- Cloud storage (S3 / ADLS / GCS)  \n",
    "- Databases (JDBC connectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data ingestion with schema inference\n",
    "df_inferred = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"/FileStore/orders.csv\")\n",
    "\n",
    "print(\"Schema with inference:\")\n",
    "df_inferred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **2. Explicit Schema Definition**\n",
    "\n",
    "### Why Use Explicit Schema?\n",
    "‚úî Predictable and reliable  \n",
    "‚úî Enforces consistent data types  \n",
    "‚úî Prevents data quality issues from type changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"order_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read with explicit schema\n",
    "df = spark.read.csv(\"/FileStore/orders.csv\", schema=schema, header=True)\n",
    "\n",
    "print(\"Schema with explicit definition:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **3. Data Exploration**\n",
    "\n",
    "### Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the first 5 rows\n",
    "print(\"First 5 rows:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "print(\"Summary statistics:\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extended summary including quartiles\n",
    "print(\"Extended summary:\")\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **4. Data Profiling and Quality Checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"Null value counts:\")\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "print(f\"Distinct rows: {df.distinct().count()}\")\n",
    "print(f\"Duplicate rows: {df.count() - df.distinct().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **5. Deeper Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by product_id and calculate total quantity\n",
    "product_sales = df.groupBy(\"product_id\") \\\n",
    "                  .agg(sum(\"quantity\").alias(\"total_quantity\"),\n",
    "                       count(\"*\").alias(\"order_count\"),\n",
    "                       avg(\"price\").alias(\"avg_price\"))\n",
    "\n",
    "print(\"Product sales summary:\")\n",
    "product_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find top products by quantity\n",
    "top_products = product_sales.orderBy(\"total_quantity\", ascending=False)\n",
    "print(\"Top 10 products by quantity:\")\n",
    "top_products.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **6. Format Conversion: CSV ‚Üí Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Parquet\n",
    "print(\"Converting to Parquet...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"/FileStore/orders_parquet\")\n",
    "\n",
    "parquet_time = time.time() - start_time\n",
    "print(f\"Parquet write completed in {parquet_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from Parquet and compare performance\n",
    "print(\"Reading from Parquet...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_parquet = spark.read.parquet(\"/FileStore/orders_parquet\")\n",
    "\n",
    "parquet_read_time = time.time() - start_time\n",
    "print(f\"Parquet read completed in {parquet_read_time:.2f} seconds\")\n",
    "\n",
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **7. Delta Lake Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Delta format\n",
    "print(\"Converting to Delta...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/orders_delta\")\n",
    "\n",
    "delta_time = time.time() - start_time\n",
    "print(f\"Delta write completed in {delta_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from Delta\n",
    "print(\"Reading from Delta...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_delta = spark.read.format(\"delta\").load(\"/FileStore/orders_delta\")\n",
    "\n",
    "delta_read_time = time.time() - start_time\n",
    "print(f\"Delta read completed in {delta_read_time:.2f} seconds\")\n",
    "\n",
    "df_delta.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **8. Delta Lake Features**\n",
    "\n",
    "### Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column for schema evolution\n",
    "df_with_region = df.withColumn(\"region\", lit(\"US\"))\n",
    "\n",
    "print(\"DataFrame with new region column:\")\n",
    "df_with_region.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write with schema evolution\n",
    "df_with_region.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/FileStore/orders_delta\")\n",
    "\n",
    "print(\"Schema evolution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View Delta table history\n",
    "delta_table = DeltaTable.forPath(spark, \"/FileStore/orders_delta\")\n",
    "print(\"Delta table history:\")\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query previous version (if available)\n",
    "try:\n",
    "    df_v0 = spark.read.format(\"delta\") \\\n",
    "                  .option(\"versionAsOf\", 0) \\\n",
    "                  .load(\"/FileStore/orders_delta\")\n",
    "    \n",
    "    print(\"Version 0 data:\")\n",
    "    df_v0.show(5)\n",
    "    print(\"Version 0 columns:\", df_v0.columns)\n",
    "except:\n",
    "    print(\"Version 0 not available or same as current version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **9. Data Quality Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for data quality issues\n",
    "print(\"Data Quality Checks:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check for negative quantities\n",
    "negative_qty = df_delta.filter(col(\"quantity\") < 0).count()\n",
    "print(f\"Records with negative quantity: {negative_qty}\")\n",
    "\n",
    "# Check for zero or negative prices\n",
    "invalid_prices = df_delta.filter(col(\"price\") <= 0).count()\n",
    "print(f\"Records with invalid prices: {invalid_prices}\")\n",
    "\n",
    "# Check for reasonable quantity ranges\n",
    "high_qty = df_delta.filter(col(\"quantity\") > 1000).count()\n",
    "print(f\"Records with unusually high quantity (>1000): {high_qty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **10. Summary and Best Practices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Week 2 Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"‚úÖ Data ingestion with explicit schema\")\n",
    "print(\"‚úÖ Data exploration and profiling\")\n",
    "print(\"‚úÖ Format conversion (CSV ‚Üí Parquet ‚Üí Delta)\")\n",
    "print(\"‚úÖ Delta Lake features (schema evolution, time travel)\")\n",
    "print(\"‚úÖ Data quality validation\")\n",
    "print(\"‚úÖ Performance comparison\")\n",
    "\n",
    "print(\"\\nBest Practices:\")\n",
    "print(\"- Use explicit schemas in production\")\n",
    "print(\"- Profile data for quality issues\")\n",
    "print(\"- Choose appropriate formats for your use case\")\n",
    "print(\"- Leverage Delta Lake for reliable pipelines\")\n",
    "print(\"- Implement data quality checks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Format Comparison Table**\n",
    "\n",
    "| Feature | CSV | Parquet | Delta |\n",
    "|---------|-----|---------|-------|\n",
    "| Structure | Row-based | Columnar | Columnar with transaction log |\n",
    "| Compression | Basic | High & efficient | High & efficient |\n",
    "| ACID Transactions | ‚ùå | ‚ùå | ‚úÖ |\n",
    "| Schema Evolution | ‚ùå | Limited | ‚úÖ |\n",
    "| Time Travel | ‚ùå | ‚ùå | ‚úÖ |\n",
    "| Use Case | Simple data sharing | Analytics & BI | Reliable data pipelines |"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Week2_Ingest_Explore_PySpark",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}