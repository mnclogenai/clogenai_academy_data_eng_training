{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 â€“ Data Cleaning & Transformation with Spark & PySpark\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- Understand Spark's execution model and how it optimizes data transformations\n",
    "- Apply ETL vs ELT concepts using the Medallion Architecture\n",
    "- Implement data cleaning and validation techniques with PySpark\n",
    "- Handle data quality issues and route invalid records appropriately\n",
    "- Troubleshoot common Spark and Delta Lake issues\n",
    "- Build a Bronze â†’ Silver transformation project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference - Essential PySpark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for data cleaning\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, lower, upper, when, isnan, isnull,\n",
    "    to_date, regexp_replace, split, concat, lit\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "\n",
    "# Common cleaning patterns\n",
    "df.withColumn(\"col_name\", trim(col(\"col_name\")))  # Remove whitespace\n",
    "df.withColumn(\"col_name\", when(col(\"col_name\") == \"\", None).otherwise(col(\"col_name\")))  # Empty to null\n",
    "df.filter(col(\"col_name\").isNotNull())  # Remove nulls\n",
    "df.withColumn(\"col_name\", col(\"col_name\").cast(\"int\"))  # Type casting\n",
    "\n",
    "# Reading from Delta tables\n",
    "spark.table(\"catalog.schema.table_name\")  # Read Delta table\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"catalog.schema.table_name\")  # Write Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Execution Model (Driver, Executors, DAG, Lazy Evaluation)\n",
    "\n",
    "### Spark Architecture (Conceptual View)\n",
    "```\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚   Notebook / UI     â”‚\n",
    "        â”‚ (Databricks, IDE)   â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â”‚ 1) Submit job\n",
    "                   â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚       Driver        â”‚\n",
    "        â”‚  â€¢ Builds DAG       â”‚\n",
    "        â”‚  â€¢ Schedules tasks  â”‚\n",
    "        â”‚  â€¢ Tracks progress  â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â”‚ 2) Requests resources\n",
    "                   â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚  Cluster Manager    â”‚\n",
    "        â”‚  (e.g., Databricks) â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚\n",
    "                   â”‚ 3) Launch executors\n",
    "                   â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚              â”‚              â”‚\n",
    "    â–¼              â–¼              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Executor1â”‚    â”‚Executor2â”‚    â”‚ExecutorNâ”‚\n",
    "â”‚â€¢ Runs   â”‚    â”‚â€¢ Runs   â”‚    â”‚â€¢ Runs   â”‚\n",
    "â”‚  tasks  â”‚    â”‚  tasks  â”‚    â”‚  tasks  â”‚\n",
    "â”‚â€¢ Stores â”‚    â”‚â€¢ Stores â”‚    â”‚â€¢ Stores â”‚\n",
    "â”‚  cache  â”‚    â”‚  cache  â”‚    â”‚  cache  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Quick Notes\n",
    "- **Driver**: The main program that coordinates the Spark application (like a project manager)\n",
    "- **Executors**: Worker nodes that actually process the data (like team members doing the work)\n",
    "- **DAG**: Directed Acyclic Graph - Spark's execution plan showing the order of operations\n",
    "- **Lazy Evaluation**: Transformations are not executed immediately, only when an action is called\n",
    "\n",
    "### Key Points\n",
    "- Spark uses a driver to coordinate work and executors to run tasks.\n",
    "- Transformations are lazy; Spark builds a DAG and executes only when an action is triggered.\n",
    "- `.explain()` shows the logical and physical plan Spark generates.\n",
    "- Common actions: count(), show(), collect(), first()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy evaluation in action - builds execution plan\n",
    "df = spark.table(\"academy.week3.bronze_sample\")\n",
    "df_transformed = df.filter(\"amount > 100\").select(\"customer\", \"amount\")\n",
    "\n",
    "# Only now does Spark execute the transformations\n",
    "df_transformed.show()       # action triggers execution\n",
    "df_transformed.explain()    # shows optimized DAG\n",
    "\n",
    "# This connects to data cleaning: chain transformations for efficiency\n",
    "cleaned_df = df \\\n",
    "    .filter(col(\"amount\").isNotNull()) \\\n",
    "    .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "    .withColumn(\"customer\", trim(col(\"customer\")))\n",
    "    # All transformations above are lazy until an action is called"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ETL vs ELT and the Medallion Architecture\n",
    "\n",
    "### Medallion Architecture Flow\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   BRONZE    â”‚â”€â”€â”€â–¶â”‚   SILVER    â”‚â”€â”€â”€â–¶â”‚    GOLD     â”‚\n",
    "â”‚             â”‚    â”‚             â”‚    â”‚             â”‚\n",
    "â”‚ Raw Data    â”‚    â”‚ Cleaned &   â”‚    â”‚ Aggregated  â”‚\n",
    "â”‚ â€¢ CSV/JSON  â”‚    â”‚ Validated   â”‚    â”‚ â€¢ Metrics   â”‚\n",
    "â”‚ â€¢ No Schema â”‚    â”‚ â€¢ Schema    â”‚    â”‚ â€¢ Reports   â”‚\n",
    "â”‚ â€¢ Ingested  â”‚    â”‚ â€¢ Quality   â”‚    â”‚ â€¢ Analytics â”‚\n",
    "â”‚   As-Is     â”‚    â”‚   Checks    â”‚    â”‚   Ready     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Data Quality Transformation Example\n",
    "```\n",
    "BEFORE (Bronze):                    AFTER (Silver):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ customer_name       â”‚            â”‚ customer_name       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ \" John Doe \"        â”‚     â”€â”€â”€â–¶   â”‚ john doe            â”‚\n",
    "â”‚ \"JANE SMITH\"        â”‚            â”‚ jane smith          â”‚\n",
    "â”‚ \"\"                  â”‚            â”‚ NULL                â”‚\n",
    "â”‚ \"Bob Jones \"        â”‚            â”‚ bob jones           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ETL vs ELT Comparison\n",
    "```\n",
    "ETL (Extract, Transform, Load)     ELT (Extract, Load, Transform)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Source â†’ Transform â†’ Target â”‚    â”‚ Source â†’ Target â†’ Transform â”‚\n",
    "â”‚                             â”‚    â”‚                             â”‚\n",
    "â”‚ â€¢ Transform outside target  â”‚    â”‚ â€¢ Transform inside target   â”‚\n",
    "â”‚ â€¢ Limited by compute power  â”‚    â”‚ â€¢ Leverage target's power   â”‚\n",
    "â”‚ â€¢ Slower for large data     â”‚    â”‚ â€¢ Faster for large data     â”‚\n",
    "â”‚ â€¢ Traditional approach      â”‚    â”‚ â€¢ Modern lakehouse approach â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Points\n",
    "- **ETL**: Transform data before loading into the target system\n",
    "- **ELT**: Load raw data first, then transform using the target system's compute power\n",
    "- **Databricks/Lakehouse**: Naturally aligns with ELT approach\n",
    "- **Benefits of ELT**: Faster ingestion, scalable transformations, data lineage preservation\n",
    "- **Medallion Layers**:\n",
    "  - Bronze â†’ raw ingested data (ELT \"Load\" step)\n",
    "  - Silver â†’ cleaned and standardized (ELT \"Transform\" step)\n",
    "  - Gold â†’ aggregated and analytics-ready (ELT \"Transform\" step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze_df = spark.table(\"academy.week3.bronze_orders\")\n",
    "\n",
    "silver_df = bronze_df \\\n",
    "    .withColumn(\"order_id\", col(\"order_id\").cast(\"int\")) \\\n",
    "    .withColumn(\"customer\", trim(col(\"customer\")))\n",
    "\n",
    "silver_df.write.mode(\"overwrite\").saveAsTable(\"academy.week3.silver_orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Standardization with PySpark\n",
    "\n",
    "### Key Points\n",
    "- Common cleaning operations: rename columns, trim whitespace, cast types, normalize text.\n",
    "- Handle nulls: fill, drop, or convert empty strings to null.\n",
    "- Use `.summary()` to validate cleaned results.\n",
    "- AI tools can help draft cleaning logic, but always validate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, lower, when, to_date\n",
    "\n",
    "df = spark.table(\"academy.week3.bronze_customers\")\n",
    "\n",
    "cleaned = df \\\n",
    "    .withColumnRenamed(\"Customer Name\", \"customer_name\") \\\n",
    "    .withColumn(\"customer_name\", trim(lower(col(\"customer_name\")))) \\\n",
    "    .withColumn(\"signup_date\", to_date(col(\"signup_date\"), \"MM/dd/yyyy\")) \\\n",
    "    .withColumn(\"rating\", col(\"rating\").cast(\"int\")) \\\n",
    "    .withColumn(\"comments\", when(col(\"comments\") == \"\", None).otherwise(col(\"comments\")))\n",
    "\n",
    "cleaned.summary().show()\n",
    "\n",
    "# Pro tip: This summary() call triggers execution of all chained transformations above\n",
    "# demonstrating lazy evaluation from Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing or Invalid Records\n",
    "\n",
    "### Key Points\n",
    "- Identify rows missing required fields.\n",
    "- Route invalid rows to a rejects storage location.\n",
    "- Silver data should include only valid and complete records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = cleaned.filter(col(\"customer_name\").isNotNull() & col(\"rating\").isNotNull())\n",
    "rejects = cleaned.filter(col(\"customer_name\").isNull() | col(\"rating\").isNull())\n",
    "\n",
    "rejects.write.mode(\"overwrite\").saveAsTable(\"academy.week3.rejects_customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Writing Silver Output in Delta Format\n",
    "\n",
    "### Key Points\n",
    "- Delta provides ACID transactions, schema enforcement, and versioning.\n",
    "- Silver datasets should be written in Delta format.\n",
    "- Always validate written results by reading them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.write.mode(\"overwrite\").saveAsTable(\"academy.week3.silver_customers\")\n",
    "\n",
    "spark.table(\"academy.week3.silver_customers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example Bronze â†’ Silver â†’ Gold Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze â†’ Silver: Data Cleaning\n",
    "bronze_df = spark.table(\"academy.week3.bronze_sales\")\n",
    "\n",
    "silver_df = bronze_df \\\n",
    "    .withColumn(\"sales\", col(\"sales\").cast(\"double\")) \\\n",
    "    .withColumn(\"product\", trim(col(\"product\"))) \\\n",
    "    .withColumn(\"sale_date\", to_date(col(\"sale_date\"), \"yyyy-MM-dd\")) \\\n",
    "    .filter(col(\"sales\").isNotNull() & col(\"product\").isNotNull())\n",
    "\n",
    "silver_df.write.mode(\"overwrite\").saveAsTable(\"academy.week3.silver_sales\")\n",
    "\n",
    "# Silver â†’ Gold: Business Aggregations\n",
    "from pyspark.sql.functions import sum, count, avg, date_format\n",
    "\n",
    "gold_df = silver_df \\\n",
    "    .withColumn(\"month\", date_format(col(\"sale_date\"), \"yyyy-MM\")) \\\n",
    "    .groupBy(\"product\", \"month\") \\\n",
    "    .agg(\n",
    "        sum(\"sales\").alias(\"total_sales\"),\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        avg(\"sales\").alias(\"avg_sale_amount\")\n",
    "    )\n",
    "\n",
    "gold_df.write.mode(\"overwrite\").saveAsTable(\"academy.week3.gold_sales_summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Points\n",
    "- Which steps belong in Bronze vs Silver vs Gold?\n",
    "- What made the final result suitable for each layer?\n",
    "- How does Gold layer serve business analytics needs?\n",
    "- How does Spark's lazy evaluation (Section 1) optimize this multi-step workflow?\n",
    "- What troubleshooting steps (Section 7) would you take if this pipeline failed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting Common Issues\n",
    "\n",
    "### Schema Mismatch Errors\n",
    "```\n",
    "Error: \"Cannot resolve column name 'customer_id' among (customer_ID, name, email)\"\n",
    "Solution: Check column names with df.columns or use case-insensitive matching\n",
    "```\n",
    "\n",
    "### Memory Issues with Large Datasets\n",
    "```\n",
    "Error: \"OutOfMemoryError\" or \"Task not serializable\"\n",
    "Solutions:\n",
    "- Avoid .collect() on large DataFrames\n",
    "- Use .show(n) instead of .collect()\n",
    "- Increase executor memory or reduce partition size\n",
    "- Use .cache() strategically for DataFrames used multiple times\n",
    "```\n",
    "\n",
    "### Performance Issues with Transformations\n",
    "```\n",
    "Error: Slow execution or hanging jobs\n",
    "Solutions:\n",
    "- Check for data skew with .describe() and .summary()\n",
    "- Use .repartition() to balance data across executors\n",
    "- Leverage lazy evaluation by chaining transformations before actions\n",
    "- Monitor Spark UI for bottlenecks\n",
    "```\n",
    "\n",
    "### Data Validation Failures\n",
    "```\n",
    "Error: Unexpected data types or values after cleaning\n",
    "Solutions:\n",
    "- Always validate with .printSchema() after transformations\n",
    "- Use .summary() to check data distributions\n",
    "- Test cleaning logic on small samples first (.limit(100))\n",
    "- Document assumptions and validate them with assertions\n",
    "```\n",
    "\n",
    "### Delta Table Conflicts\n",
    "```\n",
    "Error: \"ConcurrentModificationException\"\n",
    "Solution: Use .option(\"mergeSchema\", \"true\") for schema evolution\n",
    "```\n",
    "\n",
    "### Null Handling Issues\n",
    "```\n",
    "Error: \"NullPointerException\" during transformations\n",
    "Solutions:\n",
    "- Use .na.fill() or .na.drop() before transformations\n",
    "- Check for nulls with .filter(col(\"column\").isNotNull())\n",
    "- Use when().otherwise() for conditional logic\n",
    "```\n",
    "\n",
    "### Type Casting Failures\n",
    "```\n",
    "Error: \"NumberFormatException\" when casting strings to numbers\n",
    "Solution: Clean data first, then cast\n",
    "df.withColumn(\"amount\", regexp_replace(col(\"amount\"), \"[^0-9.]\", \"\").cast(\"double\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways & Success Criteria\n",
    "\n",
    "### What Makes Quality Silver Data?\n",
    "âœ… **Schema Consistency**: All columns have correct data types  \n",
    "âœ… **Data Completeness**: Required fields are not null  \n",
    "âœ… **Format Standardization**: Text is trimmed, normalized case  \n",
    "âœ… **Validation Passed**: Summary statistics show expected ranges  \n",
    "âœ… **Delta Format**: ACID compliance and schema enforcement enabled  \n",
    "\n",
    "### Performance Best Practices\n",
    "- **Lazy Evaluation**: Chain transformations before triggering actions\n",
    "- **Partitioning**: Consider `.repartition()` for better performance\n",
    "- **Caching**: Use `.cache()` for DataFrames used multiple times\n",
    "- **Schema Inference**: Avoid `inferSchema=True` in production\n",
    "\n",
    "### Quality Checks to Always Perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before and after counts\n",
    "bronze_df = spark.table(\"academy.week3.bronze_customers\")\n",
    "silver_df = spark.table(\"academy.week3.silver_customers\")\n",
    "rejects_df = spark.table(\"academy.week3.rejects_customers\")\n",
    "\n",
    "print(f\"Bronze records: {bronze_df.count()}\")\n",
    "print(f\"Silver records: {silver_df.count()}\")\n",
    "print(f\"Rejected records: {rejects_df.count()}\")\n",
    "\n",
    "# Data quality summary\n",
    "silver_df.summary().show()\n",
    "silver_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hands-On Project Guide\n",
    "\n",
    "### ðŸŽ¯ Your Mission\n",
    "Transform messy Bronze customer/transaction data into production-ready Silver tables that analysts can trust.\n",
    "\n",
    "### ðŸ“‹ Step-by-Step Checklist\n",
    "\n",
    "#### Phase 1: Data Discovery\n",
    "- [ ] Load Bronze data and examine with `.show(5)` and `.printSchema()`\n",
    "- [ ] Run `.summary()` to identify data quality issues\n",
    "- [ ] Document 3-5 specific problems you found\n",
    "\n",
    "#### Phase 2: Cleaning Strategy\n",
    "- [ ] Define cleaning rules for each problematic column\n",
    "- [ ] Identify which records should be rejected vs cleaned\n",
    "- [ ] Plan your transformation chain\n",
    "\n",
    "#### Phase 3: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for your cleaning pipeline\n",
    "bronze_df = spark.table(\"academy.week3.bronze_customers\")\n",
    "\n",
    "silver_df = bronze_df \\\n",
    "    .withColumnRenamed(\"old_name\", \"new_name\") \\\n",
    "    .withColumn(\"clean_col\", trim(lower(col(\"messy_col\")))) \\\n",
    "    .withColumn(\"typed_col\", col(\"string_col\").cast(\"int\")) \\\n",
    "    .filter(col(\"required_field\").isNotNull())\n",
    "\n",
    "silver_df.write.mode(\"overwrite\").saveAsTable(\"academy.week3.silver_customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase 4: Validation & Output\n",
    "- [ ] Compare before/after record counts\n",
    "- [ ] Verify data types with `.printSchema()`\n",
    "- [ ] Save Silver and rejects to Delta tables\n",
    "- [ ] Test reading data back successfully\n",
    "\n",
    "### ðŸ† Success Metrics\n",
    "- **Data Quality**: >95% of valid records preserved\n",
    "- **Schema Compliance**: All columns have correct types\n",
    "- **Documentation**: Clear explanation of cleaning decisions\n",
    "- **Reproducibility**: Code runs without errors\n",
    "\n",
    "### ðŸ“Š Expected Deliverables\n",
    "- **Silver Delta Table**: Clean, typed, validated data\n",
    "- **Rejects Delta Table**: Invalid records with reasons\n",
    "- **Quality Report**: Before/after statistics comparison\n",
    "- **Documentation**: Cleaning decisions and business rules\n",
    "\n",
    "### ðŸ’¡ Pro Tips for Your Project\n",
    "- Start with small samples using `.limit(1000)` for faster iteration\n",
    "- Use `.explain()` to understand Spark's execution plan\n",
    "- Save intermediate results to debug transformation steps\n",
    "- Always validate your assumptions with `.summary()` and `.describe()`\n",
    "- Use `spark.table()` to read from Delta tables instead of file paths\n",
    "- Use `.saveAsTable()` to write managed Delta tables with automatic optimization\n",
    "\n",
    "### ðŸš€ What You'll Learn\n",
    "By completing this project, you'll master the core skills of a data engineer:\n",
    "- **Spark Architecture**: Apply lazy evaluation and DAG optimization in real transformations\n",
    "- **Medallion Architecture**: Implement ELT patterns with Bronze â†’ Silver â†’ Gold flow\n",
    "- **Data Quality**: Use PySpark functions for cleaning, validation, and error handling\n",
    "- **Troubleshooting**: Debug common issues using Spark UI and error messages\n",
    "- **Production Skills**: Write reliable, scalable data pipelines that power business decisions\n",
    "\n",
    "### ðŸ”— How This Connects\n",
    "This project integrates all concepts from today's session:\n",
    "1. **Spark execution model** optimizes your transformation chains\n",
    "2. **ELT approach** loads raw data first, then transforms in the lakehouse\n",
    "3. **Data cleaning techniques** ensure Silver layer quality\n",
    "4. **Error handling** routes bad data to rejects tables\n",
    "5. **Troubleshooting skills** help debug real-world issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}