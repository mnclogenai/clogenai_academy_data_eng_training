# Week 3 â€“ Data Cleaning & Transformation with Spark & PySpark

## Learning Objectives
By the end of this session, you will:
- Understand Spark's execution model and how it optimizes data transformations
- Apply ETL vs ELT concepts using the Medallion Architecture
- Implement data cleaning and validation techniques with PySpark
- Handle data quality issues and route invalid records appropriately
- Troubleshoot common Spark and Delta Lake issues
- Build a Bronze â†’ Silver transformation project

## Quick Reference - Essential PySpark Functions

```python
# Core imports for data cleaning
from pyspark.sql.functions import (
    col, trim, lower, upper, when, isnan, isnull,
    to_date, regexp_replace, split, concat, lit
)
from pyspark.sql.types import IntegerType, DoubleType, StringType

# Common cleaning patterns
df.withColumn("col_name", trim(col("col_name")))  # Remove whitespace
df.withColumn("col_name", when(col("col_name") == "", None).otherwise(col("col_name")))  # Empty to null
df.filter(col("col_name").isNotNull())  # Remove nulls
df.withColumn("col_name", col("col_name").cast("int"))  # Type casting

# Reading from Delta tables
spark.table("catalog.schema.table_name")  # Read Delta table
df.write.mode("overwrite").saveAsTable("catalog.schema.table_name")  # Write Delta table
```

## 1. Spark Execution Model (Driver, Executors, DAG, Lazy Evaluation)

### Spark Architecture (Conceptual View)
```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Notebook / UI     â”‚
        â”‚ (Databricks, IDE)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ 1) Submit job
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Driver        â”‚
        â”‚  â€¢ Builds DAG       â”‚
        â”‚  â€¢ Schedules tasks  â”‚
        â”‚  â€¢ Tracks progress  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ 2) Requests resources
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Cluster Manager    â”‚
        â”‚  (e.g., Databricks) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ 3) Launch executors
                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚              â”‚
    â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Executor1â”‚    â”‚Executor2â”‚    â”‚ExecutorNâ”‚
â”‚â€¢ Runs   â”‚    â”‚â€¢ Runs   â”‚    â”‚â€¢ Runs   â”‚
â”‚  tasks  â”‚    â”‚  tasks  â”‚    â”‚  tasks  â”‚
â”‚â€¢ Stores â”‚    â”‚â€¢ Stores â”‚    â”‚â€¢ Stores â”‚
â”‚  cache  â”‚    â”‚  cache  â”‚    â”‚  cache  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quick Notes
- **Driver**: The main program that coordinates the Spark application (like a project manager)
- **Executors**: Worker nodes that actually process the data (like team members doing the work)
- **DAG**: Directed Acyclic Graph - Spark's execution plan showing the order of operations
- **Lazy Evaluation**: Transformations are not executed immediately, only when an action is called

### Key Points
- Spark uses a driver to coordinate work and executors to run tasks.
- Transformations are lazy; Spark builds a DAG and executes only when an action is triggered.
- `.explain()` shows the logical and physical plan Spark generates.
- Common actions: count(), show(), collect(), first().

### Example Code
```python
# Lazy evaluation in action - builds execution plan
df = spark.table("academy.week3.bronze_sample")
df_transformed = df.filter("amount > 100").select("customer", "amount")

# Only now does Spark execute the transformations
df_transformed.show()       # action triggers execution
df_transformed.explain()    # shows optimized DAG

# This connects to data cleaning: chain transformations for efficiency
cleaned_df = df \
    .filter(col("amount").isNotNull()) \
    .withColumn("amount", col("amount").cast("double")) \
    .withColumn("customer", trim(col("customer")))
    # All transformations above are lazy until an action is called
```

## 2. ETL vs ELT and the Medallion Architecture

### Medallion Architecture Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BRONZE    â”‚â”€â”€â”€â–¶â”‚   SILVER    â”‚â”€â”€â”€â–¶â”‚    GOLD     â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â”‚ Raw Data    â”‚    â”‚ Cleaned &   â”‚    â”‚ Aggregated  â”‚
â”‚ â€¢ CSV/JSON  â”‚    â”‚ Validated   â”‚    â”‚ â€¢ Metrics   â”‚
â”‚ â€¢ No Schema â”‚    â”‚ â€¢ Schema    â”‚    â”‚ â€¢ Reports   â”‚
â”‚ â€¢ Ingested  â”‚    â”‚ â€¢ Quality   â”‚    â”‚ â€¢ Analytics â”‚
â”‚   As-Is     â”‚    â”‚   Checks    â”‚    â”‚   Ready     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Quality Transformation Example
```
BEFORE (Bronze):                    AFTER (Silver):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ customer_name       â”‚            â”‚ customer_name       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ " John Doe "        â”‚     â”€â”€â”€â–¶   â”‚ john doe            â”‚
â”‚ "JANE SMITH"        â”‚            â”‚ jane smith          â”‚
â”‚ ""                  â”‚            â”‚ NULL                â”‚
â”‚ "Bob Jones "        â”‚            â”‚ bob jones           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ETL vs ELT Comparison
```
ETL (Extract, Transform, Load)     ELT (Extract, Load, Transform)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Source â†’ Transform â†’ Target â”‚    â”‚ Source â†’ Target â†’ Transform â”‚
â”‚                             â”‚    â”‚                             â”‚
â”‚ â€¢ Transform outside target  â”‚    â”‚ â€¢ Transform inside target   â”‚
â”‚ â€¢ Limited by compute power  â”‚    â”‚ â€¢ Leverage target's power   â”‚
â”‚ â€¢ Slower for large data     â”‚    â”‚ â€¢ Faster for large data     â”‚
â”‚ â€¢ Traditional approach      â”‚    â”‚ â€¢ Modern lakehouse approach â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Points
- **ETL**: Transform data before loading into the target system
- **ELT**: Load raw data first, then transform using the target system's compute power
- **Databricks/Lakehouse**: Naturally aligns with ELT approach
- **Benefits of ELT**: Faster ingestion, scalable transformations, data lineage preservation
- **Medallion Layers**:
  - Bronze â†’ raw ingested data (ELT "Load" step)
  - Silver â†’ cleaned and standardized (ELT "Transform" step)
  - Gold â†’ aggregated and analytics-ready (ELT "Transform" step)

### Example Code
```python
bronze_df = spark.table("academy.week3.bronze_orders")

silver_df = bronze_df \
    .withColumn("order_id", col("order_id").cast("int")) \
    .withColumn("customer", trim(col("customer")))

silver_df.write.mode("overwrite").saveAsTable("academy.week3.silver_orders")

```

## 3. Data Cleaning & Standardization with PySpark

### Key Points
- Common cleaning operations: rename columns, trim whitespace, cast types, normalize text.
- Handle nulls: fill, drop, or convert empty strings to null.
- Use `.summary()` to validate cleaned results.
- AI tools can help draft cleaning logic, but always validate output.

### Example Code
```python
from pyspark.sql.functions import col, trim, lower, when, to_date

df = spark.table("academy.week3.bronze_customers")

cleaned = df \
    .withColumnRenamed("Customer Name", "customer_name") \
    .withColumn("customer_name", trim(lower(col("customer_name")))) \
    .withColumn("signup_date", to_date(col("signup_date"), "MM/dd/yyyy")) \
    .withColumn("rating", col("rating").cast("int")) \
    .withColumn("comments", when(col("comments") == "", None).otherwise(col("comments")))

cleaned.summary().show()

# Pro tip: This summary() call triggers execution of all chained transformations above
# demonstrating lazy evaluation from Section 1
```
## 4. Handling Missing or Invalid Records

### Key Points
- Identify rows missing required fields.
- Route invalid rows to a rejects storage location.
- Silver data should include only valid and complete records.

### Example Code
```python
valid = cleaned.filter(col("customer_name").isNotNull() & col("rating").isNotNull())
rejects = cleaned.filter(col("customer_name").isNull() | col("rating").isNull())

rejects.write.mode("overwrite").saveAsTable("academy.week3.rejects_customers")

```
## 5. Writing Silver Output in Delta Format

### Key Points
- Delta provides ACID transactions, schema enforcement, and versioning.
- Silver datasets should be written in Delta format.
- Always validate written results by reading them back.

### Example Code
```python
valid.write.mode("overwrite").saveAsTable("academy.week3.silver_customers")

spark.table("academy.week3.silver_customers").show()

```
## 6. Example End-to-End Bronze â†’ Silver â†’ Gold Workflow

### Example Code
```python
# Bronze â†’ Silver: Data Cleaning
bronze_df = spark.table("academy.week3.bronze_sales")

silver_df = bronze_df \
    .withColumn("sales", col("sales").cast("double")) \
    .withColumn("product", trim(col("product"))) \
    .withColumn("sale_date", to_date(col("sale_date"), "yyyy-MM-dd")) \
    .filter(col("sales").isNotNull() & col("product").isNotNull())

silver_df.write.mode("overwrite").saveAsTable("academy.week3.silver_sales")

# Silver â†’ Gold: Business Aggregations
from pyspark.sql.functions import sum, count, avg, date_format

gold_df = silver_df \
    .withColumn("month", date_format(col("sale_date"), "yyyy-MM")) \
    .groupBy("product", "month") \
    .agg(
        sum("sales").alias("total_sales"),
        count("*").alias("transaction_count"),
        avg("sales").alias("avg_sale_amount")
    )

gold_df.write.mode("overwrite").saveAsTable("academy.week3.gold_sales_summary")
```

### Discussion Points
- Which steps belong in Bronze vs Silver vs Gold?
- What made the final result suitable for each layer?
- How does Gold layer serve business analytics needs?
- How does Spark's lazy evaluation (Section 1) optimize this multi-step workflow?
- What troubleshooting steps (Section 7) would you take if this pipeline failed?

## 7. Troubleshooting Common Issues

### Schema Mismatch Errors
```
Error: "Cannot resolve column name 'customer_id' among (customer_ID, name, email)"
Solution: Check column names with df.columns or use case-insensitive matching
```

### Memory Issues with Large Datasets
```
Error: "OutOfMemoryError" or "Task not serializable"
Solutions:
- Avoid .collect() on large DataFrames
- Use .show(n) instead of .collect()
- Increase executor memory or reduce partition size
- Use .cache() strategically for DataFrames used multiple times
```

### Performance Issues with Transformations
```
Error: Slow execution or hanging jobs
Solutions:
- Check for data skew with .describe() and .summary()
- Use .repartition() to balance data across executors
- Leverage lazy evaluation by chaining transformations before actions
- Monitor Spark UI for bottlenecks
```

### Data Validation Failures
```
Error: Unexpected data types or values after cleaning
Solutions:
- Always validate with .printSchema() after transformations
- Use .summary() to check data distributions
- Test cleaning logic on small samples first (.limit(100))
- Document assumptions and validate them with assertions
```ollect()
- Increase executor memory or reduce partition size
```

### Delta Table Conflicts
```
Error: "ConcurrentModificationException"
Solution: Use .option("mergeSchema", "true") for schema evolution
```

### Null Handling Issues
```
Error: "NullPointerException" during transformations
Solutions:
- Use .na.fill() or .na.drop() before transformations
- Check for nulls with .filter(col("column").isNotNull())
- Use when().otherwise() for conditional logic
```

### Type Casting Failures
```
Error: "NumberFormatException" when casting strings to numbers
Solution: Clean data first, then cast
df.withColumn("amount", regexp_replace(col("amount"), "[^0-9.]", "").cast("double"))
```

## 8. Key Takeaways & Success Criteria

### What Makes Quality Silver Data?
âœ… **Schema Consistency**: All columns have correct data types  
âœ… **Data Completeness**: Required fields are not null  
âœ… **Format Standardization**: Text is trimmed, normalized case  
âœ… **Validation Passed**: Summary statistics show expected ranges  
âœ… **Delta Format**: ACID compliance and schema enforcement enabled  

### Performance Best Practices
- **Lazy Evaluation**: Chain transformations before triggering actions
- **Partitioning**: Consider `.repartition()` for better performance
- **Caching**: Use `.cache()` for DataFrames used multiple times
- **Schema Inference**: Avoid `inferSchema=True` in production

### Quality Checks to Always Perform
```python
# Before and after counts
bronze_df = spark.table("academy.week3.bronze_customers")
silver_df = spark.table("academy.week3.silver_customers")
rejects_df = spark.table("academy.week3.rejects_customers")

print(f"Bronze records: {bronze_df.count()}")
print(f"Silver records: {silver_df.count()}")
print(f"Rejected records: {rejects_df.count()}")

# Data quality summary
silver_df.summary().show()
silver_df.printSchema()
```

## 9. Hands-On Project Guide

### ðŸŽ¯ Your Mission
Transform messy Bronze customer/transaction data into production-ready Silver tables that analysts can trust.

### ðŸ“‹ Step-by-Step Checklist

#### Phase 1: Data Discovery
- [ ] Load Bronze data and examine with `.show(5)` and `.printSchema()`
- [ ] Run `.summary()` to identify data quality issues
- [ ] Document 3-5 specific problems you found

#### Phase 2: Cleaning Strategy
- [ ] Define cleaning rules for each problematic column
- [ ] Identify which records should be rejected vs cleaned
- [ ] Plan your transformation chain

#### Phase 3: Implementation
```python
# Template for your cleaning pipeline
bronze_df = spark.table("academy.week3.bronze_customers")

silver_df = bronze_df \
    .withColumnRenamed("old_name", "new_name") \
    .withColumn("clean_col", trim(lower(col("messy_col")))) \
    .withColumn("typed_col", col("string_col").cast("int")) \
    .filter(col("required_field").isNotNull())

silver_df.write.mode("overwrite").saveAsTable("academy.week3.silver_customers")
```

#### Phase 4: Validation & Output
- [ ] Compare before/after record counts
- [ ] Verify data types with `.printSchema()`
- [ ] Save Silver and rejects to Delta tables
- [ ] Test reading data back successfully

### ðŸ† Success Metrics
- **Data Quality**: >95% of valid records preserved
- **Schema Compliance**: All columns have correct types
- **Documentation**: Clear explanation of cleaning decisions
- **Reproducibility**: Code runs without errors

### ðŸ“Š Expected Deliverables
- **Silver Delta Table**: Clean, typed, validated data
- **Rejects Delta Table**: Invalid records with reasons
- **Quality Report**: Before/after statistics comparison
- **Documentation**: Cleaning decisions and business rules

### ðŸ’¡ Pro Tips for Your Project
- Start with small samples using `.limit(1000)` for faster iteration
- Use `.explain()` to understand Spark's execution plan
- Save intermediate results to debug transformation steps
- Always validate your assumptions with `.summary()` and `.describe()`
- Use `spark.table()` to read from Delta tables instead of file paths
- Use `.saveAsTable()` to write managed Delta tables with automatic optimization

### ðŸš€ What You'll Learn
By completing this project, you'll master the core skills of a data engineer:
- **Spark Architecture**: Apply lazy evaluation and DAG optimization in real transformations
- **Medallion Architecture**: Implement ELT patterns with Bronze â†’ Silver â†’ Gold flow
- **Data Quality**: Use PySpark functions for cleaning, validation, and error handling
- **Troubleshooting**: Debug common issues using Spark UI and error messages
- **Production Skills**: Write reliable, scalable data pipelines that power business decisions

### ðŸ”— How This Connects
This project integrates all concepts from today's session:
1. **Spark execution model** optimizes your transformation chains
2. **ELT approach** loads raw data first, then transforms in the lakehouse
3. **Data cleaning techniques** ensure Silver layer quality
4. **Error handling** routes bad data to rejects tables
5. **Troubleshooting skills** help debug real-world issues