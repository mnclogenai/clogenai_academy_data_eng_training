{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 ‚Äì PySpark Transformations & Modeling\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- üîÑ Apply schema evolution and enforcement with Delta Lake\n",
    "- üöÄ Build incremental pipelines using MERGE and timestamp-based logic\n",
    "- üìä Use joins, aggregations, and window functions for analytical metrics\n",
    "- üèÜ Create Gold-layer tables for revenue, retention, and customer metrics\n",
    "- ü§ñ Use Databricks AI to define schemas and generate PySpark + SQL DDL\n",
    "- ‚úÖ Validate business-friendly analytical models in Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Schema Evolution, Enforcement & Incremental Delta Processing\n",
    "\n",
    "### Why Schema Evolution Matters\n",
    "Modern data systems receive changing datasets: new columns, modified types, updated structures.\n",
    "\n",
    "Delta Lake's schema evolution + enforcement ensures:\n",
    "- ‚úÖ Reliable ingestion even when schemas shift\n",
    "- üõ°Ô∏è Protection against invalid/incorrect data\n",
    "- üîß Ability to process changing feeds without rebuilding pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Basic Schema Change Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load different versions of orders data\n",
    "df1 = spark.table(\"bronze.orders_v1\")\n",
    "df2 = spark.table(\"bronze.orders_v2\")\n",
    "\n",
    "print(\"Orders V1 Schema:\")\n",
    "df1.printSchema()\n",
    "\n",
    "print(\"\\nOrders V2 Schema:\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable schema evolution when writing to Delta table\n",
    "df2.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"silver.orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Table History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Delta table history\n",
    "delta_tbl = DeltaTable.forName(spark, \"silver.orders\")\n",
    "delta_tbl.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Incremental Data Processing & MERGE Operations\n",
    "\n",
    "### When to Use Incremental Pipelines\n",
    "**Use Cases:**\n",
    "- üîÑ Continuous ingestion\n",
    "- ‚è∞ Late-arriving events\n",
    "- üíº Updated business transactions\n",
    "- üìä Slowly changing dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical Incremental MERGE Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load incremental data\n",
    "incremental_df = spark.table(\"bronze.orders_incremental\")\n",
    "delta_tbl = DeltaTable.forName(spark, \"silver.orders\")\n",
    "\n",
    "# Perform MERGE operation\n",
    "delta_tbl.alias(\"t\").merge(\n",
    "    incremental_df.alias(\"s\"),\n",
    "    \"t.order_id = s.order_id\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "print(\"MERGE operation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Joins, Aggregations & Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Customer and Order Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tables\n",
    "orders = spark.table(\"silver.orders\")\n",
    "customers = spark.table(\"silver.customers\")\n",
    "\n",
    "# Join data\n",
    "joined = orders.join(customers, \"customer_id\", \"left\")\n",
    "joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations for Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate customer metrics\n",
    "metrics = orders.groupBy(\"customer_id\").agg(\n",
    "    sum(\"amount\").alias(\"total_revenue\"),\n",
    "    count(\"*\").alias(\"order_count\"),\n",
    "    avg(\"amount\").alias(\"avg_order_value\")\n",
    ")\n",
    "\n",
    "metrics.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions\n",
    "Powerful for retention, running totals, LTV, and ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window specification\n",
    "w = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "# Apply window functions\n",
    "df = orders.withColumn(\"prev_order\", lag(\"order_date\").over(w)) \\\n",
    "           .withColumn(\"lifetime_spend\", sum(\"amount\").over(w))\n",
    "\n",
    "df.select(\"customer_id\", \"order_date\", \"amount\", \"prev_order\", \"lifetime_spend\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Designing Gold-Layer Tables for Analytics\n",
    "\n",
    "### Why Gold Tables Exist\n",
    "Gold tables serve business stakeholders:\n",
    "- üí∞ Revenue metrics\n",
    "- üîÑ Retention indicators\n",
    "- üìä Customer activity signals\n",
    "- üìà Aggregates and KPIs\n",
    "- üéØ Dashboard-ready structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Gold Schema Definition (AI-assisted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gold table schema\n",
    "ddl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS gold.customer_metrics (\n",
    "    customer_id STRING,\n",
    "    total_revenue DOUBLE,\n",
    "    avg_order_value DOUBLE,\n",
    "    order_count INT,\n",
    "    lifetime_value DOUBLE,\n",
    "    retention_flag BOOLEAN,\n",
    "    first_order_date DATE,\n",
    "    latest_order_date DATE\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(ddl)\n",
    "print(\"Gold table created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gold Table Example: Revenue & Retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive gold metrics\n",
    "gold = joined.groupBy(\"customer_id\").agg(\n",
    "    sum(\"amount\").alias(\"total_revenue\"),\n",
    "    avg(\"amount\").alias(\"avg_order_value\"),\n",
    "    count(\"*\").alias(\"order_count\"),\n",
    "    min(\"order_date\").alias(\"first_order_date\"),\n",
    "    max(\"order_date\").alias(\"latest_order_date\")\n",
    ").withColumn(\"lifetime_value\", col(\"total_revenue\")) \\\n",
    " .withColumn(\"retention_flag\", col(\"order_count\") > 1)\n",
    "\n",
    "gold.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Gold Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Gold table\n",
    "gold.write.mode(\"overwrite\").saveAsTable(\"gold.customer_metrics\")\n",
    "print(\"Gold table populated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Incremental Customer Metrics Pipeline\n",
    "\n",
    "Build a pipeline that:\n",
    "1. üîÑ Handles schema evolution on multiple versions of an orders feed\n",
    "2. üöÄ Processes incremental updates using MERGE\n",
    "3. üìä Produces customer metrics using joins, aggregations & window functions\n",
    "4. üèÜ Stores results in a Gold Delta table\n",
    "5. ü§ñ Uses Databricks AI to define schema + generate DDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1 ‚Äî Schema Evolution üîÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load V1, V2, V3 order files\n",
    "# TODO: Apply schema evolution\n",
    "# TODO: Inspect table history\n",
    "# TODO: Document column changes\n",
    "\n",
    "print(\"Phase 1 - Schema Evolution: TODO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 ‚Äî Incremental MERGE Logic üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify incremental key or timestamp\n",
    "# TODO: Build MERGE logic\n",
    "# TODO: Validate row counts\n",
    "\n",
    "print(\"Phase 2 - Incremental MERGE Logic: TODO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3 ‚Äî Metrics Development üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Join orders + customers\n",
    "# TODO: Create grouped aggregations\n",
    "# TODO: Add window-based metrics (LTV, retention)\n",
    "\n",
    "print(\"Phase 3 - Metrics Development: TODO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4 ‚Äî Gold Table üèÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use AI to propose schema\n",
    "# TODO: Generate PySpark + SQL DDL\n",
    "# TODO: Create and populate Gold table\n",
    "\n",
    "print(\"Phase 4 - Gold Table: TODO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5 ‚Äî Validation ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Validate schema\n",
    "# TODO: Validate row counts\n",
    "# TODO: Show final records\n",
    "\n",
    "print(\"Phase 5 - Validation: TODO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting Guide\n",
    "\n",
    "### Common Issues & Fixes\n",
    "\n",
    "#### ‚ùå Schema mismatch\n",
    "```\n",
    "Error: Cannot write incompatible data to column\n",
    "```\n",
    "**Fix**: Enable mergeSchema or correct types before write.\n",
    "\n",
    "#### ‚ùå Many-to-many join explosion\n",
    "**Fix**: Deduplicate keys or validate join keys before joining.\n",
    "\n",
    "#### ‚ùå Window functions fail\n",
    "**Fix**: Ensure orderBy uses a consistent type (date, timestamp).\n",
    "\n",
    "#### ‚ùå MERGE slow\n",
    "**Fix**: Z-ORDER and OPTIMIZE the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Optimize Delta table\n",
    "spark.sql(\"OPTIMIZE gold.customer_metrics ZORDER BY customer_id\")\n",
    "print(\"Table optimized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways & Success Criteria\n",
    "\n",
    "### What You Should Understand\n",
    "- üîÑ Delta's schema evolution enables flexibility\n",
    "- üí∞ Incremental pipelines reduce compute costs\n",
    "- üìä Joins + aggregations + windows create analytical signals\n",
    "- üèÜ Gold tables provide consistent business metrics\n",
    "- ü§ñ AI speeds up schema design & DDL creation\n",
    "\n",
    "### Your Notebook Must Include\n",
    "- ‚úÖ Working schema evolution demo\n",
    "- ‚úÖ MERGE incremental logic\n",
    "- ‚úÖ Customer metrics (revenue, order count, LTV)\n",
    "- ‚úÖ Gold table creation + preview\n",
    "- ‚úÖ Reflection on learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reflection & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation\n",
    "print(\"=== Final Pipeline Validation ===\")\n",
    "print(f\"Gold table row count: {spark.table('gold.customer_metrics').count()}\")\n",
    "print(\"\\nSample records:\")\n",
    "spark.table('gold.customer_metrics').show(5)\n",
    "\n",
    "print(\"\\nüéâ Week 4 Pipeline Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
