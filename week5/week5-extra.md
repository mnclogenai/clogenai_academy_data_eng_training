# Data Quality & Validation

## Overview

This guide provides comprehensive coverage of data quality management and validation techniques in modern data engineering. It focuses on identifying quality issues, implementing validation frameworks, and building robust quality assurance pipelines using Great Expectations, Delta constraints, and monitoring best practices.

## Learning Objectives
By the end of this guide, you will:
- ðŸ” Master data quality dimensions and issue identification techniques
- ðŸ“‹ Design comprehensive validation rules using multiple approaches
- ðŸŽ¯ Implement Great Expectations Core for declarative validation
- ðŸ›¡ï¸ Apply Delta Lake constraints for database-level enforcement
- ðŸ“Š Build data quality monitoring and profiling systems
- âœ… Create end-to-end quality assurance pipelines

---

## 1. Data Quality Dimensions & Issue Identification

### Understanding Data Quality Dimensions

**The Six Pillars of Data Quality**

**Completeness** - Ensuring all required data is present
- Missing values in critical fields
- Incomplete records or partial data loads
- Null values where business rules require data

**Accuracy** - Data correctly represents real-world entities
- Incorrect customer information
- Wrong calculations or derived values
- Outdated or stale information

**Validity** - Data conforms to defined formats and business rules
- Invalid email formats or phone numbers
- Values outside acceptable ranges
- Incorrect data types or formats

**Uniqueness** - No unwanted duplicate records
- Duplicate customer records
- Multiple entries for same transaction
- Primary key violations

**Consistency** - Data is uniform across systems and time
- Different formats for same data across sources
- Conflicting information between systems
- Inconsistent naming conventions

**Timeliness** - Data is current and available when needed
- Delayed data delivery
- Outdated information
- Missing recent transactions

### Systematic Quality Assessment

**Automated Quality Profiling**
```python
from pyspark.sql.functions import *
from pyspark.sql.types import *

def comprehensive_data_profile(df, table_name):
    """Generate comprehensive data quality profile"""
    
    total_rows = df.count()
    total_columns = len(df.columns)
    
    # Basic statistics
    profile_results = {
        'table_name': table_name,
        'total_rows': total_rows,
        'total_columns': total_columns,
        'column_profiles': {}
    }
    
    for column in df.columns:
        col_type = dict(df.dtypes)[column]
        
        # Completeness metrics
        null_count = df.filter(col(column).isNull()).count()
        completeness_rate = (total_rows - null_count) / total_rows * 100
        
        # Uniqueness metrics
        distinct_count = df.select(column).distinct().count()
        uniqueness_rate = distinct_count / total_rows * 100
        
        column_profile = {
            'data_type': col_type,
            'null_count': null_count,
            'completeness_rate': round(completeness_rate, 2),
            'distinct_count': distinct_count,
            'uniqueness_rate': round(uniqueness_rate, 2)
        }
        
        # Type-specific analysis
        if col_type in ['int', 'bigint', 'double', 'float']:
            stats = df.select(
                min(column).alias('min_val'),
                max(column).alias('max_val'),
                avg(column).alias('avg_val'),
                stddev(column).alias('std_dev')
            ).collect()[0]
            
            column_profile.update({
                'min_value': stats.min_val,
                'max_value': stats.max_val,
                'average': round(stats.avg_val, 2) if stats.avg_val else None,
                'std_deviation': round(stats.std_dev, 2) if stats.std_dev else None
            })
        
        elif col_type == 'string':
            string_stats = df.select(
                min(length(column)).alias('min_length'),
                max(length(column)).alias('max_length'),
                avg(length(column)).alias('avg_length')
            ).collect()[0]
            
            column_profile.update({
                'min_length': string_stats.min_length,
                'max_length': string_stats.max_length,
                'avg_length': round(string_stats.avg_length, 2) if string_stats.avg_length else None
            })
        
        profile_results['column_profiles'][column] = column_profile
    
    return profile_results
```

**Quality Issue Detection**
```python
def detect_quality_issues(df):
    """Detect common data quality issues"""
    
    issues = []
    
    # Check for completely empty columns
    for column in df.columns:
        null_count = df.filter(col(column).isNull()).count()
        if null_count == df.count():
            issues.append({
                'type': 'COMPLETENESS',
                'severity': 'CRITICAL',
                'column': column,
                'description': f'Column {column} is completely empty'
            })
    
    # Check for high null rates
    for column in df.columns:
        null_rate = df.filter(col(column).isNull()).count() / df.count()
        if null_rate > 0.5:  # More than 50% nulls
            issues.append({
                'type': 'COMPLETENESS',
                'severity': 'HIGH',
                'column': column,
                'description': f'Column {column} has {null_rate:.1%} null values'
            })
    
    # Check for duplicate records
    total_rows = df.count()
    distinct_rows = df.distinct().count()
    if total_rows != distinct_rows:
        duplicate_count = total_rows - distinct_rows
        issues.append({
            'type': 'UNIQUENESS',
            'severity': 'MEDIUM',
            'column': 'ALL',
            'description': f'Found {duplicate_count} duplicate records'
        })
    
    # Check for invalid email formats (if email column exists)
    if 'email' in df.columns:
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        invalid_emails = df.filter(
            col('email').isNotNull() & 
            ~col('email').rlike(email_pattern)
        ).count()
        
        if invalid_emails > 0:
            issues.append({
                'type': 'VALIDITY',
                'severity': 'MEDIUM',
                'column': 'email',
                'description': f'Found {invalid_emails} invalid email formats'
            })
    
    # Check for negative amounts (if amount column exists)
    if 'amount' in df.columns:
        negative_amounts = df.filter(col('amount') < 0).count()
        if negative_amounts > 0:
            issues.append({
                'type': 'VALIDITY',
                'severity': 'HIGH',
                'column': 'amount',
                'description': f'Found {negative_amounts} negative amounts'
            })
    
    return issues
```

---

## 2. Schema Enforcement & Rule-Based Validation

### Schema Definition and Enforcement

**Strict Schema Enforcement**
```python
from pyspark.sql.types import *

def define_strict_schema():
    """Define comprehensive schema with constraints"""
    
    return StructType([
        StructField("order_id", StringType(), False),  # Not nullable
        StructField("customer_id", StringType(), False),
        StructField("product_id", StringType(), True),  # Nullable
        StructField("amount", DecimalType(10, 2), False),
        StructField("quantity", IntegerType(), False),
        StructField("order_date", DateType(), False),
        StructField("status", StringType(), False),
        StructField("created_at", TimestampType(), False),
        StructField("updated_at", TimestampType(), True)
    ])

def enforce_schema_on_table(table_name, schema):
    """Enforce schema when reading from table with error handling"""
    
    try:
        df = spark.table(table_name)
        validation_results = validate_schema_compatibility(df, schema)
        
        if not validation_results['is_compatible']:
            raise ValueError(f"Schema validation failed: {validation_results}")
        
        return df, None
        
    except Exception as e:
        # Handle schema mismatch gracefully
        error_details = {
            'error_type': 'SCHEMA_MISMATCH',
            'message': str(e),
            'table_name': table_name
        }
        return None, error_details
```

**Dynamic Schema Validation**
```python
def validate_schema_compatibility(df, expected_schema):
    """Validate DataFrame schema against expected schema"""
    
    actual_fields = {field.name: field for field in df.schema.fields}
    expected_fields = {field.name: field for field in expected_schema.fields}
    
    validation_results = {
        'is_compatible': True,
        'missing_columns': [],
        'extra_columns': [],
        'type_mismatches': [],
        'nullable_violations': []
    }
    
    # Check for missing required columns
    for field_name, field in expected_fields.items():
        if field_name not in actual_fields:
            validation_results['missing_columns'].append(field_name)
            validation_results['is_compatible'] = False
        else:
            actual_field = actual_fields[field_name]
            
            # Check data type compatibility
            if actual_field.dataType != field.dataType:
                validation_results['type_mismatches'].append({
                    'column': field_name,
                    'expected': str(field.dataType),
                    'actual': str(actual_field.dataType)
                })
                validation_results['is_compatible'] = False
            
            # Check nullable constraints
            if not field.nullable and actual_field.nullable:
                validation_results['nullable_violations'].append(field_name)
                validation_results['is_compatible'] = False
    
    # Check for extra columns
    for field_name in actual_fields:
        if field_name not in expected_fields:
            validation_results['extra_columns'].append(field_name)
    
    return validation_results
```

### Business Rule Validation

**Comprehensive Rule Engine**
```python
class DataValidationRules:
    """Comprehensive data validation rule engine"""
    
    def __init__(self):
        self.rules = {}
        self.validation_results = []
    
    def add_rule(self, rule_name, rule_function, severity='MEDIUM'):
        """Add validation rule to the engine"""
        self.rules[rule_name] = {
            'function': rule_function,
            'severity': severity
        }
    
    def validate_positive_amounts(self, df):
        """Validate that amounts are positive"""
        if 'amount' not in df.columns:
            return True, "Amount column not found"
        
        invalid_count = df.filter(col('amount') <= 0).count()
        is_valid = invalid_count == 0
        
        message = f"Found {invalid_count} non-positive amounts" if not is_valid else "All amounts are positive"
        return is_valid, message
    
    def validate_email_format(self, df):
        """Validate email format using regex"""
        if 'email' not in df.columns:
            return True, "Email column not found"
        
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        invalid_count = df.filter(
            col('email').isNotNull() & 
            ~col('email').rlike(email_pattern)
        ).count()
        
        is_valid = invalid_count == 0
        message = f"Found {invalid_count} invalid email formats" if not is_valid else "All emails are valid"
        return is_valid, message
    
    def validate_date_ranges(self, df):
        """Validate that dates are within reasonable ranges"""
        if 'order_date' not in df.columns:
            return True, "Order date column not found"
        
        # Check for future dates
        future_dates = df.filter(col('order_date') > current_date()).count()
        
        # Check for very old dates (before 1900)
        old_dates = df.filter(col('order_date') < lit('1900-01-01')).count()
        
        is_valid = future_dates == 0 and old_dates == 0
        
        issues = []
        if future_dates > 0:
            issues.append(f"{future_dates} future dates")
        if old_dates > 0:
            issues.append(f"{old_dates} dates before 1900")
        
        message = f"Date issues: {', '.join(issues)}" if issues else "All dates are valid"
        return is_valid, message
    
    def run_all_validations(self, df, **kwargs):
        """Run all registered validation rules"""
        results = []
        
        for rule_name, rule_config in self.rules.items():
            try:
                is_valid, message = rule_config['function'](df, **kwargs)
                
                results.append({
                    'rule_name': rule_name,
                    'is_valid': is_valid,
                    'message': message,
                    'severity': rule_config['severity'],
                    'timestamp': datetime.now()
                })
                
            except Exception as e:
                results.append({
                    'rule_name': rule_name,
                    'is_valid': False,
                    'message': f"Rule execution failed: {str(e)}",
                    'severity': 'CRITICAL',
                    'timestamp': datetime.now()
                })
        
        return results

# Usage example
validator = DataValidationRules()
validator.add_rule('positive_amounts', validator.validate_positive_amounts, 'HIGH')
validator.add_rule('email_format', validator.validate_email_format, 'MEDIUM')
validator.add_rule('date_ranges', validator.validate_date_ranges, 'HIGH')

validation_results = validator.run_all_validations(df)
```

---

## 3. Great Expectations Implementation

### Core Great Expectations Setup

**Environment Configuration**
```python
import great_expectations as gx
from great_expectations.core import ExpectationSuite, ExpectationConfiguration

def setup_great_expectations_context():
    """Set up Great Expectations data context"""
    
    # Initialize context
    context = gx.get_context()
    
    # Configure data source for Spark
    datasource_config = {
        "name": "spark_datasource",
        "class_name": "Datasource",
        "execution_engine": {
            "class_name": "SparkDFExecutionEngine"
        },
        "data_connectors": {
            "default_runtime_data_connector": {
                "class_name": "RuntimeDataConnector",
                "batch_identifiers": ["default_identifier_name"]
            }
        }
    }
    
    context.add_datasource(**datasource_config)
    return context
```

**Expectation Suite Creation**
```python
def create_comprehensive_expectation_suite(suite_name="data_quality_suite"):
    """Create comprehensive expectation suite for data validation"""
    
    context = setup_great_expectations_context()
    
    # Create expectation suite
    suite = context.create_expectation_suite(
        expectation_suite_name=suite_name,
        overwrite_existing=True
    )
    
    # Completeness expectations
    suite.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_not_be_null",
            kwargs={"column": "order_id"}
        )
    )
    
    suite.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_not_be_null",
            kwargs={"column": "customer_id"}
        )
    )
    
    # Uniqueness expectations
    suite.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_be_unique",
            kwargs={"column": "order_id"}
        )
    )
    
    # Range expectations
    suite.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_be_between",
            kwargs={
                "column": "amount",
                "min_value": 0,
                "max_value": 100000
            }
        )
    )
    
    # Format expectations
    suite.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_match_regex",
            kwargs={
                "column": "email",
                "regex": r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
            }
        )
    )
    
    # Set membership expectations
    suite.add_expectation(
        ExpectationConfiguration(
            expectation_type="expect_column_values_to_be_in_set",
            kwargs={
                "column": "status",
                "value_set": ["pending", "processing", "completed", "cancelled"]
            }
        )
    )
    
    # Save suite
    context.save_expectation_suite(suite)
    return suite
```

**Validation Execution**
```python
def run_great_expectations_validation(df, context, suite_name):
    """Execute Great Expectations validation on DataFrame"""
    
    # Create batch request
    batch_request = {
        "datasource_name": "spark_datasource",
        "data_connector_name": "default_runtime_data_connector",
        "data_asset_name": "validation_batch",
        "runtime_parameters": {"batch_data": df},
        "batch_identifiers": {"default_identifier_name": "validation_run"}
    }
    
    # Get validator
    validator = context.get_validator(
        batch_request=batch_request,
        expectation_suite_name=suite_name
    )
    
    # Run validation
    validation_result = validator.validate()
    
    # Process results
    results_summary = {
        'success': validation_result.success,
        'total_expectations': len(validation_result.results),
        'successful_expectations': sum(1 for r in validation_result.results if r.success),
        'failed_expectations': sum(1 for r in validation_result.results if not r.success),
        'success_rate': sum(1 for r in validation_result.results if r.success) / len(validation_result.results) * 100
    }
    
    # Extract failed expectations details
    failed_expectations = []
    for result in validation_result.results:
        if not result.success:
            failed_expectations.append({
                'expectation_type': result.expectation_config.expectation_type,
                'column': result.expectation_config.kwargs.get('column', 'N/A'),
                'observed_value': result.result.get('observed_value'),
                'details': result.result
            })
    
    return validation_result, results_summary, failed_expectations
```

---

## 4. Delta Lake Constraints & Database-Level Validation

### Delta Constraint Implementation

**Table Creation with Constraints**
```python
def create_table_with_constraints():
    """Create Delta table with comprehensive constraints"""
    
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS silver.orders_validated (
        order_id STRING NOT NULL,
        customer_id STRING NOT NULL,
        product_id STRING,
        amount DECIMAL(10,2) NOT NULL,
        quantity INTEGER NOT NULL,
        order_date DATE NOT NULL,
        status STRING NOT NULL,
        created_at TIMESTAMP NOT NULL,
        updated_at TIMESTAMP,
        
        -- Check constraints
        CONSTRAINT positive_amount CHECK (amount > 0),
        CONSTRAINT valid_quantity CHECK (quantity > 0),
        CONSTRAINT valid_status CHECK (status IN ('pending', 'processing', 'completed', 'cancelled')),
        CONSTRAINT logical_dates CHECK (updated_at >= created_at OR updated_at IS NULL),
        CONSTRAINT reasonable_amount CHECK (amount <= 100000)
    ) 
    USING DELTA
    PARTITIONED BY (DATE(order_date))
    """
    
    spark.sql(create_table_sql)
```

**Dynamic Constraint Management**
```python
def manage_delta_constraints(table_name):
    """Manage Delta table constraints dynamically"""
    
    # Add constraints to existing table
    constraints = [
        ("positive_amount", "amount > 0"),
        ("valid_email", "email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'"),
        ("future_date_check", "order_date <= current_date()"),
        ("valid_status", "status IN ('pending', 'processing', 'completed', 'cancelled')")
    ]
    
    for constraint_name, constraint_condition in constraints:
        try:
            spark.sql(f"""
                ALTER TABLE {table_name} 
                ADD CONSTRAINT {constraint_name} 
                CHECK ({constraint_condition})
            """)
            print(f"âœ… Added constraint: {constraint_name}")
            
        except Exception as e:
            print(f"âŒ Failed to add constraint {constraint_name}: {str(e)}")
    
    # List all constraints
    constraints_df = spark.sql(f"DESCRIBE DETAIL {table_name}")
    return constraints_df

def validate_before_constraint_addition(df, constraint_condition):
    """Validate data before adding constraint to avoid failures"""
    
    # Test constraint on existing data
    violating_records = df.filter(f"NOT ({constraint_condition})")
    violation_count = violating_records.count()
    
    if violation_count > 0:
        print(f"âš ï¸  Warning: {violation_count} records violate constraint: {constraint_condition}")
        return False, violating_records
    else:
        print(f"âœ… All records satisfy constraint: {constraint_condition}")
        return True, None
```

---

## 5. Data Quality Monitoring & Profiling

### Automated Quality Monitoring

**Quality Metrics Dashboard**
```python
def create_quality_metrics_dashboard():
    """Create comprehensive quality metrics for monitoring"""
    
    # Enable table monitoring
    spark.sql("""
        ALTER TABLE silver.orders_validated 
        SET TBLPROPERTIES (
            'delta.enableChangeDataFeed' = 'true'
        )
    """)
    
    # Create quality metrics table
    quality_metrics_sql = """
    CREATE TABLE IF NOT EXISTS monitoring.data_quality_metrics (
        table_name STRING,
        metric_name STRING,
        metric_value DOUBLE,
        metric_timestamp TIMESTAMP,
        partition_date DATE
    ) USING DELTA
    PARTITIONED BY (partition_date)
    """
    
    spark.sql(quality_metrics_sql)

def calculate_daily_quality_metrics(table_name, target_date=None):
    """Calculate daily quality metrics for a table"""
    
    if target_date is None:
        target_date = datetime.now().date()
    
    df = spark.table(table_name)
    
    # Filter for specific date if partitioned
    if 'order_date' in df.columns:
        df = df.filter(col('order_date') == target_date)
    
    total_records = df.count()
    
    metrics = []
    
    # Completeness metrics
    for column in df.columns:
        null_count = df.filter(col(column).isNull()).count()
        completeness_rate = (total_records - null_count) / total_records if total_records > 0 else 0
        
        metrics.append({
            'table_name': table_name,
            'metric_name': f'{column}_completeness_rate',
            'metric_value': completeness_rate,
            'metric_timestamp': datetime.now(),
            'partition_date': target_date
        })
    
    # Convert to DataFrame and save
    metrics_df = spark.createDataFrame(metrics)
    metrics_df.write.format("delta").mode("append").saveAsTable("monitoring.data_quality_metrics")
    
    return metrics_df
```

**Anomaly Detection**
```python
def detect_quality_anomalies(table_name, lookback_days=30):
    """Detect anomalies in data quality metrics"""
    
    # Get historical metrics
    historical_metrics = spark.sql(f"""
        SELECT 
            metric_name,
            metric_value,
            metric_timestamp,
            AVG(metric_value) OVER (
                PARTITION BY metric_name 
                ORDER BY metric_timestamp 
                ROWS BETWEEN {lookback_days} PRECEDING AND 1 PRECEDING
            ) as rolling_avg,
            STDDEV(metric_value) OVER (
                PARTITION BY metric_name 
                ORDER BY metric_timestamp 
                ROWS BETWEEN {lookback_days} PRECEDING AND 1 PRECEDING
            ) as rolling_stddev
        FROM monitoring.data_quality_metrics
        WHERE table_name = '{table_name}'
        AND metric_timestamp >= current_date() - INTERVAL {lookback_days + 1} DAYS
        ORDER BY metric_name, metric_timestamp
    """)
    
    # Identify anomalies (values outside 2 standard deviations)
    anomalies = historical_metrics.filter(
        (col('metric_value') > col('rolling_avg') + 2 * col('rolling_stddev')) |
        (col('metric_value') < col('rolling_avg') - 2 * col('rolling_stddev'))
    )
    
    return anomalies
```

---

## 6. End-to-End Quality Pipeline Implementation

### Complete Quality Pipeline

**Pipeline Architecture**
```python
class DataQualityPipeline:
    """Comprehensive data quality pipeline"""
    
    def __init__(self, config):
        self.config = config
        self.ge_context = setup_great_expectations_context()
        self.validation_results = []
        self.quality_metrics = {}
    
    def run_pipeline(self, df, table_name):
        """Execute complete quality pipeline"""
        
        pipeline_results = {
            'table_name': table_name,
            'start_time': datetime.now(),
            'total_records': df.count(),
            'stages': {}
        }
        
        try:
            # Stage 1: Schema validation
            schema_results = self.validate_schema(df)
            pipeline_results['stages']['schema_validation'] = schema_results
            
            if not schema_results['is_valid']:
                raise ValueError(f"Schema validation failed: {schema_results['errors']}")
            
            # Stage 2: Business rule validation
            rule_results = self.validate_business_rules(df)
            pipeline_results['stages']['business_rules'] = rule_results
            
            # Stage 3: Great Expectations validation
            ge_results = self.run_great_expectations(df)
            pipeline_results['stages']['great_expectations'] = ge_results
            
            # Stage 4: Data profiling
            profile_results = self.profile_data(df)
            pipeline_results['stages']['data_profiling'] = profile_results
            
            # Stage 5: Write with Delta constraints
            write_results = self.write_with_constraints(df, table_name)
            pipeline_results['stages']['delta_write'] = write_results
            
            # Stage 6: Quality monitoring
            monitoring_results = self.update_quality_monitoring(table_name)
            pipeline_results['stages']['quality_monitoring'] = monitoring_results
            
            pipeline_results['end_time'] = datetime.now()
            pipeline_results['duration_seconds'] = (
                pipeline_results['end_time'] - pipeline_results['start_time']
            ).total_seconds()
            pipeline_results['overall_success'] = True
            
        except Exception as e:
            pipeline_results['end_time'] = datetime.now()
            pipeline_results['error'] = str(e)
            pipeline_results['overall_success'] = False
        
        # Save pipeline results
        self.save_pipeline_results(pipeline_results)
        
        return pipeline_results
    
    def validate_schema(self, df):
        """Validate DataFrame schema"""
        expected_schema = self.config.get('expected_schema')
        if expected_schema:
            return validate_schema_compatibility(df, expected_schema)
        return {'is_valid': True, 'message': 'No schema validation configured'}
    
    def validate_business_rules(self, df):
        """Execute business rule validation"""
        validator = DataValidationRules()
        
        # Add configured rules
        for rule_name, rule_config in self.config.get('business_rules', {}).items():
            validator.add_rule(rule_name, rule_config['function'], rule_config['severity'])
        
        return validator.run_all_validations(df)
    
    def run_great_expectations(self, df):
        """Execute Great Expectations validation"""
        suite_name = self.config.get('ge_suite_name')
        if suite_name:
            validation_result, summary, failed_expectations = run_great_expectations_validation(
                df, self.ge_context, suite_name
            )
            return {
                'success': validation_result.success,
                'summary': summary,
                'failed_expectations': failed_expectations
            }
        return {'success': True, 'message': 'No Great Expectations suite configured'}
    
    def write_with_constraints(self, df, table_name):
        """Write data with Delta constraints"""
        try:
            df.write.format("delta").mode("append").saveAsTable(table_name)
            return {'success': True, 'records_written': df.count()}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def save_pipeline_results(self, results):
        """Save pipeline execution results"""
        results_df = spark.createDataFrame([results])
        results_df.write.format("delta").mode("append").saveAsTable("monitoring.pipeline_execution_log")
```

## Conclusion

Effective data quality management requires a multi-layered approach combining proactive validation, reactive monitoring, and continuous improvement. Key success factors include:

**Comprehensive Validation Strategy**
- Multiple validation layers (schema, business rules, statistical checks)
- Appropriate tools for different validation needs
- Performance optimization for large-scale data

**Robust Quality Framework**
- Great Expectations for declarative validation
- Delta constraints for database-level enforcement
- Custom business rule engines for domain-specific logic

**Operational Excellence**
- Automated quality monitoring and alerting
- Quality metrics tracking and trend analysis
- Error handling and recovery mechanisms

By implementing these patterns and following the outlined best practices, teams can build reliable data quality pipelines that ensure data trustworthiness and support confident decision-making across the organization.