{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 ‚Äî Dataflows, Lineage, Orchestration & Automation\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- üîç Master Unity Catalog lineage for end-to-end data traceability\n",
    "- üöÄ Build declarative pipelines using Delta Live Tables\n",
    "- üìä Implement comprehensive data quality expectations\n",
    "- üîÑ Design multi-task workflows with Databricks Jobs\n",
    "- üåê Establish Git-based development and deployment workflows\n",
    "- ‚úÖ Apply observability best practices for production systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unity Catalog Lineage & Data Governance\n",
    "\n",
    "### Exploring Data Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query table-level lineage\n",
    "lineage_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        source_table_full_name,\n",
    "        target_table_full_name,\n",
    "        source_type,\n",
    "        target_type,\n",
    "        created_at\n",
    "    FROM system.access.table_lineage \n",
    "    WHERE target_table_full_name LIKE '%orders%'\n",
    "    ORDER BY created_at DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(lineage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query column-level lineage\n",
    "column_lineage = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        source_table_full_name,\n",
    "        source_column_name,\n",
    "        target_table_full_name,\n",
    "        target_column_name,\n",
    "        transformation_type\n",
    "    FROM system.access.column_lineage \n",
    "    WHERE target_table_full_name LIKE '%customer%'\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "display(column_lineage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineage Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_table_dependencies(table_name):\n",
    "    \"\"\"Analyze upstream and downstream dependencies\"\"\"\n",
    "    \n",
    "    # Get upstream dependencies\n",
    "    upstream = spark.sql(f\"\"\"\n",
    "        SELECT source_table_full_name, source_type\n",
    "        FROM system.access.table_lineage \n",
    "        WHERE target_table_full_name = '{table_name}'\n",
    "    \"\"\")\n",
    "    \n",
    "    # Get downstream dependencies  \n",
    "    downstream = spark.sql(f\"\"\"\n",
    "        SELECT target_table_full_name, target_type\n",
    "        FROM system.access.table_lineage \n",
    "        WHERE source_table_full_name = '{table_name}'\n",
    "    \"\"\")\n",
    "    \n",
    "    return {\n",
    "        'upstream_count': upstream.count(),\n",
    "        'downstream_count': downstream.count(),\n",
    "        'upstream_tables': [row.source_table_full_name for row in upstream.collect()],\n",
    "        'downstream_tables': [row.target_table_full_name for row in downstream.collect()]\n",
    "    }\n",
    "\n",
    "# Test the function\n",
    "# dependencies = analyze_table_dependencies('your_catalog.your_schema.your_table')\n",
    "# print(dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_impact_analysis(source_table):\n",
    "    \"\"\"Perform comprehensive impact analysis\"\"\"\n",
    "    \n",
    "    impact_query = f\"\"\"\n",
    "    WITH RECURSIVE lineage_tree AS (\n",
    "        -- Base case: direct dependencies\n",
    "        SELECT target_table_full_name as table_name, 1 as level\n",
    "        FROM system.access.table_lineage \n",
    "        WHERE source_table_full_name = '{source_table}'\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        -- Recursive case: indirect dependencies\n",
    "        SELECT tl.target_table_full_name, lt.level + 1\n",
    "        FROM system.access.table_lineage tl\n",
    "        JOIN lineage_tree lt ON tl.source_table_full_name = lt.table_name\n",
    "        WHERE lt.level < 10  -- Prevent infinite recursion\n",
    "    )\n",
    "    SELECT table_name, level, COUNT(*) as impact_count\n",
    "    FROM lineage_tree \n",
    "    GROUP BY table_name, level\n",
    "    ORDER BY level, table_name\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(impact_query)\n",
    "\n",
    "# Test impact analysis\n",
    "# impact_result = perform_impact_analysis('your_catalog.your_schema.source_table')\n",
    "# display(impact_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classification and Governance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply data classification tags (example - adjust table name)\n",
    "# spark.sql(\"\"\"\n",
    "#     ALTER TABLE your_catalog.your_schema.customer_table \n",
    "#     SET TAGS ('classification' = 'PII', 'retention' = '7_years')\n",
    "# \"\"\")\n",
    "\n",
    "# Query tables by classification\n",
    "classified_tables = spark.sql(\"\"\"\n",
    "    SELECT table_name, table_catalog, table_schema, table_comment\n",
    "    FROM system.information_schema.tables \n",
    "    WHERE table_comment LIKE '%PII%' OR table_comment LIKE '%sensitive%'\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(classified_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor data access patterns\n",
    "access_audit = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        user_identity.email,\n",
    "        request_params.full_name_arg,\n",
    "        event_time,\n",
    "        action_name\n",
    "    FROM system.access.audit \n",
    "    WHERE action_name = 'read' \n",
    "    AND event_time >= current_date() - INTERVAL 7 DAYS\n",
    "    ORDER BY event_time DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "display(access_audit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Delta Live Tables (DLT) Pipeline Development\n",
    "\n",
    "### Creating Sample Data for DLT Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample bronze data for DLT demonstration\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generate sample orders data\n",
    "sample_orders = []\n",
    "for i in range(1000):\n",
    "    sample_orders.append((\n",
    "        f\"order_{i+1}\",\n",
    "        f\"customer_{random.randint(1, 100)}\",\n",
    "        f\"product_{random.randint(1, 50)}\",\n",
    "        round(random.uniform(10, 1000), 2),\n",
    "        random.randint(1, 5),\n",
    "        (datetime.now() - timedelta(days=random.randint(0, 365))).date(),\n",
    "        random.choice([\"pending\", \"completed\", \"cancelled\"]),\n",
    "        datetime.now()\n",
    "    ))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"order_date\", DateType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"created_at\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "bronze_orders_df = spark.createDataFrame(sample_orders, schema)\n",
    "\n",
    "# Save as bronze table\n",
    "bronze_orders_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze.sample_orders\")\n",
    "\n",
    "print(f\"Created bronze.sample_orders with {bronze_orders_df.count()} records\")\n",
    "display(bronze_orders_df.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLT Pipeline SQL Examples\n",
    "\n",
    "**Note**: The following SQL would be used in a DLT pipeline notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLT SQL Examples (for reference - would be in DLT pipeline)\n",
    "dlt_bronze_sql = \"\"\"\n",
    "-- Bronze Layer\n",
    "CREATE OR REFRESH STREAMING LIVE TABLE bronze_orders\n",
    "COMMENT \"Raw order data from source systems\"\n",
    "AS SELECT \n",
    "    *,\n",
    "    current_timestamp() as ingestion_time\n",
    "FROM STREAM(bronze.sample_orders)\n",
    "\"\"\"\n",
    "\n",
    "dlt_silver_sql = \"\"\"\n",
    "-- Silver Layer with Data Quality\n",
    "CREATE OR REFRESH LIVE TABLE silver_orders (\n",
    "    CONSTRAINT valid_order_id EXPECT (order_id IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "    CONSTRAINT positive_amount EXPECT (amount > 0) ON VIOLATION FAIL UPDATE,\n",
    "    CONSTRAINT valid_date EXPECT (order_date <= current_date()) ON VIOLATION DROP ROW\n",
    ")\n",
    "COMMENT \"Cleaned and validated order data\"\n",
    "AS SELECT \n",
    "    order_id,\n",
    "    customer_id,\n",
    "    CAST(amount AS DECIMAL(10,2)) as amount,\n",
    "    CAST(order_date AS DATE) as order_date,\n",
    "    status,\n",
    "    created_at\n",
    "FROM LIVE.bronze_orders\n",
    "WHERE order_id IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "dlt_gold_sql = \"\"\"\n",
    "-- Gold Layer Business Metrics\n",
    "CREATE OR REFRESH LIVE TABLE gold_customer_metrics\n",
    "COMMENT \"Customer lifetime value and behavior metrics\"\n",
    "AS SELECT \n",
    "    customer_id,\n",
    "    COUNT(*) as total_orders,\n",
    "    SUM(amount) as total_revenue,\n",
    "    AVG(amount) as avg_order_value,\n",
    "    MIN(order_date) as first_order_date,\n",
    "    MAX(order_date) as last_order_date,\n",
    "    DATEDIFF(MAX(order_date), MIN(order_date)) as customer_lifespan_days\n",
    "FROM LIVE.silver_orders\n",
    "GROUP BY customer_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"DLT SQL Examples:\")\n",
    "print(\"1. Bronze Layer:\", dlt_bronze_sql[:100] + \"...\")\n",
    "print(\"2. Silver Layer:\", dlt_silver_sql[:100] + \"...\")\n",
    "print(\"3. Gold Layer:\", dlt_gold_sql[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python DLT Implementation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python DLT Example (for reference - would be in DLT pipeline)\n",
    "dlt_python_example = \"\"\"\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Enhanced customer data with derived attributes\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_fail(\"valid_email\", \"email RLIKE '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$'\")\n",
    "@dlt.expect_or_drop(\"future_signup\", \"signup_date <= current_date()\")\n",
    "def silver_customers():\n",
    "    return (\n",
    "        dlt.read(\"bronze_customers\")\n",
    "        .withColumn(\"email_domain\", regexp_extract(\"email\", \"@(.+)\", 1))\n",
    "        .withColumn(\"customer_age_days\", datediff(current_date(), \"signup_date\"))\n",
    "        .withColumn(\"customer_segment\", \n",
    "            when(col(\"total_orders\") > 10, \"High Value\")\n",
    "            .when(col(\"total_orders\") > 5, \"Medium Value\")\n",
    "            .otherwise(\"New Customer\")\n",
    "        )\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "print(\"Python DLT Implementation Example:\")\n",
    "print(dlt_python_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating Silver and Gold Layer Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Silver layer processing with data quality checks\n",
    "bronze_df = spark.table(\"bronze.sample_orders\")\n",
    "\n",
    "# Apply data quality transformations\n",
    "silver_df = bronze_df.filter(\n",
    "    (col(\"order_id\").isNotNull()) &\n",
    "    (col(\"amount\") > 0) &\n",
    "    (col(\"order_date\") <= current_date())\n",
    ").withColumn(\n",
    "    \"amount_decimal\", col(\"amount\").cast(\"decimal(10,2)\")\n",
    ").select(\n",
    "    \"order_id\", \"customer_id\", \"product_id\", \n",
    "    col(\"amount_decimal\").alias(\"amount\"),\n",
    "    \"quantity\", \"order_date\", \"status\", \"created_at\"\n",
    ")\n",
    "\n",
    "# Save silver table\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.orders\")\n",
    "\n",
    "print(f\"Silver layer: {silver_df.count()} records (filtered from {bronze_df.count()})\")\n",
    "display(silver_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gold layer customer metrics\n",
    "gold_customer_metrics = silver_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    sum(\"amount\").alias(\"total_revenue\"),\n",
    "    avg(\"amount\").alias(\"avg_order_value\"),\n",
    "    min(\"order_date\").alias(\"first_order_date\"),\n",
    "    max(\"order_date\").alias(\"last_order_date\")\n",
    ").withColumn(\n",
    "    \"customer_lifespan_days\",\n",
    "    datediff(col(\"last_order_date\"), col(\"first_order_date\"))\n",
    ").withColumn(\n",
    "    \"customer_segment\",\n",
    "    when(col(\"total_revenue\") > 1000, \"High Value\")\n",
    "    .when(col(\"total_revenue\") > 500, \"Medium Value\")\n",
    "    .otherwise(\"Standard\")\n",
    ")\n",
    "\n",
    "# Save gold table\n",
    "gold_customer_metrics.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.customer_metrics\")\n",
    "\n",
    "print(f\"Gold layer: {gold_customer_metrics.count()} customer records\")\n",
    "display(gold_customer_metrics.orderBy(col(\"total_revenue\").desc()).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Databricks Jobs & Workflow Orchestration\n",
    "\n",
    "### Job Configuration Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-task job configuration example\n",
    "job_config = {\n",
    "    \"name\": \"customer_analytics_pipeline\",\n",
    "    \"email_notifications\": {\n",
    "        \"on_failure\": [\"data-team@company.com\"],\n",
    "        \"on_success\": [\"stakeholders@company.com\"]\n",
    "    },\n",
    "    \"timeout_seconds\": 3600,\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"ingest_raw_data\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/pipelines/01_data_ingestion\",\n",
    "                \"base_parameters\": {\"source_table\": \"bronze.orders\"}\n",
    "            },\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"i3.xlarge\",\n",
    "                \"num_workers\": 2\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"process_silver_layer\",\n",
    "            \"depends_on\": [{\"task_key\": \"ingest_raw_data\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/pipelines/02_silver_processing\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"create_gold_metrics\",\n",
    "            \"depends_on\": [{\"task_key\": \"process_silver_layer\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/pipelines/03_gold_aggregations\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Job Configuration:\")\n",
    "import json\n",
    "print(json.dumps(job_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task dependency management\n",
    "task_dependencies = {\n",
    "    \"bronze_ingestion\": [],  # No dependencies\n",
    "    \"silver_customers\": [\"bronze_ingestion\"],\n",
    "    \"silver_orders\": [\"bronze_ingestion\"],\n",
    "    \"gold_customer_metrics\": [\"silver_customers\", \"silver_orders\"],\n",
    "    \"gold_revenue_analysis\": [\"silver_orders\"],\n",
    "    \"final_reporting\": [\"gold_customer_metrics\", \"gold_revenue_analysis\"]\n",
    "}\n",
    "\n",
    "def create_job_with_dependencies(tasks, dependencies):\n",
    "    \"\"\"Create job configuration with proper task dependencies\"\"\"\n",
    "    \n",
    "    job_tasks = []\n",
    "    for task_name, deps in dependencies.items():\n",
    "        task_config = {\n",
    "            \"task_key\": task_name,\n",
    "            \"notebook_task\": {\"notebook_path\": f\"/pipelines/{task_name}\"},\n",
    "            \"depends_on\": [{\"task_key\": dep} for dep in deps]\n",
    "        }\n",
    "        job_tasks.append(task_config)\n",
    "    \n",
    "    return {\"tasks\": job_tasks}\n",
    "\n",
    "# Generate job configuration\n",
    "dependency_job = create_job_with_dependencies([], task_dependencies)\n",
    "print(\"Task Dependencies:\")\n",
    "for task in dependency_job[\"tasks\"]:\n",
    "    deps = [dep[\"task_key\"] for dep in task[\"depends_on\"]]\n",
    "    print(f\"{task['task_key']}: depends on {deps if deps else 'nothing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling and Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduling configurations\n",
    "scheduling_configs = {\n",
    "    \"daily_schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 0 2 * * ?\",  # Daily at 2 AM\n",
    "        \"timezone_id\": \"UTC\"\n",
    "    },\n",
    "    \"business_hours_schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 0 9-17 * * MON-FRI\",  # Hourly during business hours\n",
    "        \"timezone_id\": \"America/New_York\"\n",
    "    },\n",
    "    \"table_trigger\": {\n",
    "        \"table_update\": {\n",
    "            \"table_names\": [\"bronze.orders\", \"bronze.customers\"],\n",
    "            \"condition\": \"ANY\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Error handling configurations\n",
    "error_handling_configs = {\n",
    "    \"retry_config\": {\n",
    "        \"max_retries\": 3,\n",
    "        \"min_retry_interval_millis\": 60000,  # 1 minute\n",
    "        \"retry_on_timeout\": True\n",
    "    },\n",
    "    \"notification_config\": {\n",
    "        \"email_notifications\": {\n",
    "            \"on_start\": [\"team@company.com\"],\n",
    "            \"on_success\": [\"stakeholders@company.com\"],\n",
    "            \"on_failure\": [\"oncall@company.com\", \"team@company.com\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Scheduling Configurations:\")\n",
    "print(json.dumps(scheduling_configs, indent=2))\n",
    "print(\"\\nError Handling Configurations:\")\n",
    "print(json.dumps(error_handling_configs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Environment Configuration & Git Integration\n",
    "\n",
    "### Environment-Specific Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration examples\n",
    "environment_configs = {\n",
    "    \"dev\": {\n",
    "        \"environment\": \"development\",\n",
    "        \"database\": {\n",
    "            \"catalog\": \"dev_catalog\",\n",
    "            \"schema\": \"analytics\"\n",
    "        },\n",
    "        \"storage\": {\n",
    "            \"root_table\": \"bronze.dev_orders\"\n",
    "        },\n",
    "        \"compute\": {\n",
    "            \"cluster_size\": \"small\",\n",
    "            \"auto_terminate\": True\n",
    "        },\n",
    "        \"notifications\": {\n",
    "            \"enabled\": False\n",
    "        }\n",
    "    },\n",
    "    \"prod\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"database\": {\n",
    "            \"catalog\": \"prod_catalog\",\n",
    "            \"schema\": \"analytics\"\n",
    "        },\n",
    "        \"storage\": {\n",
    "            \"root_table\": \"bronze.orders\"\n",
    "        },\n",
    "        \"compute\": {\n",
    "            \"cluster_size\": \"large\",\n",
    "            \"auto_terminate\": False\n",
    "        },\n",
    "        \"notifications\": {\n",
    "            \"enabled\": True,\n",
    "            \"channels\": [\"#data-alerts\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_environment_config(env_name):\n",
    "    \"\"\"Load environment-specific configuration\"\"\"\n",
    "    return environment_configs.get(env_name, environment_configs[\"dev\"])\n",
    "\n",
    "# Example usage\n",
    "current_env = \"dev\"  # This would come from a widget or parameter\n",
    "config = load_environment_config(current_env)\n",
    "\n",
    "print(f\"Configuration for {current_env} environment:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Observability & Monitoring\n",
    "\n",
    "### System Tables Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job execution patterns\n",
    "job_metrics = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        job_name,\n",
    "        DATE(start_time) as execution_date,\n",
    "        COUNT(*) as total_runs,\n",
    "        SUM(CASE WHEN result_state = 'SUCCESS' THEN 1 ELSE 0 END) as successful_runs,\n",
    "        AVG(execution_duration) as avg_duration_seconds,\n",
    "        MAX(execution_duration) as max_duration_seconds\n",
    "    FROM system.lakeflow.job_runs \n",
    "    WHERE start_time >= current_date() - INTERVAL 7 DAYS\n",
    "    GROUP BY job_name, DATE(start_time)\n",
    "    ORDER BY execution_date DESC, job_name\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"Job Execution Metrics:\")\n",
    "display(job_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor DLT pipeline health\n",
    "dlt_metrics = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pipeline_name,\n",
    "        update_id,\n",
    "        state,\n",
    "        start_time,\n",
    "        end_time,\n",
    "        (end_time - start_time) as duration_seconds\n",
    "    FROM system.lakeflow.pipeline_updates \n",
    "    WHERE start_time >= current_date() - INTERVAL 24 HOURS\n",
    "    ORDER BY start_time DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"DLT Pipeline Metrics:\")\n",
    "display(dlt_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track data quality results\n",
    "quality_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        dataset_name,\n",
    "        expectation_name,\n",
    "        DATE(timestamp) as check_date,\n",
    "        COUNT(*) as total_checks,\n",
    "        SUM(CASE WHEN passed THEN 1 ELSE 0 END) as passed_checks,\n",
    "        (SUM(CASE WHEN passed THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) as pass_rate\n",
    "    FROM system.lakeflow.data_quality_results\n",
    "    WHERE timestamp >= current_date() - INTERVAL 30 DAYS\n",
    "    GROUP BY dataset_name, expectation_name, DATE(timestamp)\n",
    "    HAVING pass_rate < 95  -- Alert on low pass rates\n",
    "    ORDER BY check_date DESC, pass_rate ASC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data Quality Trends (Issues Only):\")\n",
    "display(quality_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Monitoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_health_dashboard():\n",
    "    \"\"\"Create comprehensive pipeline health metrics\"\"\"\n",
    "    \n",
    "    # Job success rates\n",
    "    job_health = spark.sql(\"\"\"\n",
    "        WITH job_stats AS (\n",
    "            SELECT \n",
    "                job_name,\n",
    "                COUNT(*) as total_runs,\n",
    "                SUM(CASE WHEN result_state = 'SUCCESS' THEN 1 ELSE 0 END) as success_count\n",
    "            FROM system.lakeflow.job_runs \n",
    "            WHERE start_time >= current_date() - INTERVAL 7 DAYS\n",
    "            GROUP BY job_name\n",
    "        )\n",
    "        SELECT \n",
    "            job_name,\n",
    "            total_runs,\n",
    "            success_count,\n",
    "            (success_count * 100.0 / total_runs) as success_rate,\n",
    "            CASE \n",
    "                WHEN (success_count * 100.0 / total_runs) >= 95 THEN 'Healthy'\n",
    "                WHEN (success_count * 100.0 / total_runs) >= 80 THEN 'Warning'\n",
    "                ELSE 'Critical'\n",
    "            END as health_status\n",
    "        FROM job_stats\n",
    "        ORDER BY success_rate ASC\n",
    "    \"\"\")\n",
    "    \n",
    "    return job_health\n",
    "\n",
    "# Generate pipeline health report\n",
    "health_report = create_pipeline_health_dashboard()\n",
    "print(\"Pipeline Health Dashboard:\")\n",
    "display(health_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_health_alerts(health_df):\n",
    "    \"\"\"Generate alerts for pipeline health issues\"\"\"\n",
    "    \n",
    "    critical_jobs = health_df.filter(col(\"health_status\") == \"Critical\")\n",
    "    warning_jobs = health_df.filter(col(\"health_status\") == \"Warning\")\n",
    "    \n",
    "    alerts = []\n",
    "    \n",
    "    for job in critical_jobs.collect():\n",
    "        alerts.append({\n",
    "            \"severity\": \"CRITICAL\",\n",
    "            \"message\": f\"Job {job.job_name} has {job.success_rate:.1f}% success rate\",\n",
    "            \"job_name\": job.job_name,\n",
    "            \"success_rate\": job.success_rate\n",
    "        })\n",
    "    \n",
    "    for job in warning_jobs.collect():\n",
    "        alerts.append({\n",
    "            \"severity\": \"WARNING\",\n",
    "            \"message\": f\"Job {job.job_name} has {job.success_rate:.1f}% success rate\",\n",
    "            \"job_name\": job.job_name,\n",
    "            \"success_rate\": job.success_rate\n",
    "        })\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# Generate alerts\n",
    "alerts = generate_health_alerts(health_report)\n",
    "print(f\"Generated {len(alerts)} alerts:\")\n",
    "for alert in alerts:\n",
    "    print(f\"[{alert['severity']}] {alert['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Troubleshooting & Debugging\n",
    "\n",
    "### Common Issue Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_lineage_issues(table_name):\n",
    "    \"\"\"Diagnose common lineage tracking problems\"\"\"\n",
    "    \n",
    "    # Check if table exists in Unity Catalog\n",
    "    table_exists = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as count \n",
    "        FROM system.information_schema.tables \n",
    "        WHERE table_name = '{table_name}'\n",
    "    \"\"\").collect()[0].count > 0\n",
    "    \n",
    "    if not table_exists:\n",
    "        return \"Table not registered in Unity Catalog\"\n",
    "    \n",
    "    # Check for recent lineage updates\n",
    "    recent_lineage = spark.sql(f\"\"\"\n",
    "        SELECT MAX(created_at) as last_update\n",
    "        FROM system.access.table_lineage \n",
    "        WHERE target_table_full_name LIKE '%{table_name}%'\n",
    "    \"\"\").collect()[0].last_update\n",
    "    \n",
    "    if recent_lineage is None:\n",
    "        return \"No lineage information found - check if operations use Unity Catalog\"\n",
    "    \n",
    "    return \"Lineage tracking appears normal\"\n",
    "\n",
    "# Test lineage diagnosis\n",
    "diagnosis = diagnose_lineage_issues(\"orders\")\n",
    "print(f\"Lineage diagnosis: {diagnosis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_dlt_pipeline_failures(pipeline_id):\n",
    "    \"\"\"Debug common DLT pipeline issues\"\"\"\n",
    "    \n",
    "    # Get recent pipeline events\n",
    "    pipeline_events = spark.sql(f\"\"\"\n",
    "        SELECT event_type, message, timestamp, details\n",
    "        FROM system.lakeflow.pipeline_events \n",
    "        WHERE pipeline_id = '{pipeline_id}'\n",
    "        AND timestamp >= current_timestamp() - INTERVAL 24 HOURS\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT 100\n",
    "    \"\"\")\n",
    "    \n",
    "    # Analyze error patterns\n",
    "    error_summary = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            event_type,\n",
    "            COUNT(*) as error_count,\n",
    "            COLLECT_LIST(message) as error_messages\n",
    "        FROM system.lakeflow.pipeline_events \n",
    "        WHERE pipeline_id = '{pipeline_id}'\n",
    "        AND event_type LIKE '%ERROR%'\n",
    "        AND timestamp >= current_timestamp() - INTERVAL 24 HOURS\n",
    "        GROUP BY event_type\n",
    "        ORDER BY error_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    return pipeline_events, error_summary\n",
    "\n",
    "# Example usage (would need actual pipeline_id)\n",
    "print(\"DLT debugging function ready - use with actual pipeline_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def troubleshoot_job_failures(job_name, hours_back=24):\n",
    "    \"\"\"Analyze job failure patterns and root causes\"\"\"\n",
    "    \n",
    "    failure_analysis = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            run_id,\n",
    "            start_time,\n",
    "            end_time,\n",
    "            result_state,\n",
    "            error_message\n",
    "        FROM system.lakeflow.job_runs \n",
    "        WHERE job_name = '{job_name}'\n",
    "        AND start_time >= current_timestamp() - INTERVAL {hours_back} HOURS\n",
    "        AND result_state IN ('FAILED', 'TIMEOUT', 'CANCELLED')\n",
    "        ORDER BY start_time DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    # Common failure patterns\n",
    "    failure_patterns = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN error_message LIKE '%OutOfMemoryError%' THEN 'Memory Issues'\n",
    "                WHEN error_message LIKE '%timeout%' THEN 'Timeout Issues'\n",
    "                WHEN error_message LIKE '%TableNotFoundException%' THEN 'Table Availability'\n",
    "                WHEN error_message LIKE '%AnalysisException%' THEN 'Schema Issues'\n",
    "                ELSE 'Other'\n",
    "            END as failure_category,\n",
    "            COUNT(*) as occurrence_count,\n",
    "            COLLECT_LIST(DISTINCT error_message) as sample_errors\n",
    "        FROM system.lakeflow.job_runs \n",
    "        WHERE job_name = '{job_name}'\n",
    "        AND start_time >= current_timestamp() - INTERVAL {hours_back} HOURS\n",
    "        AND result_state = 'FAILED'\n",
    "        GROUP BY failure_category\n",
    "        ORDER BY occurrence_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    return failure_analysis, failure_patterns\n",
    "\n",
    "# Example usage (would need actual job_name)\n",
    "print(\"Job troubleshooting function ready - use with actual job_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Best Practices\n",
    "\n",
    "### Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of what we've covered\n",
    "summary = {\n",
    "    \"Unity Catalog Lineage\": {\n",
    "        \"capabilities\": [\n",
    "            \"Table-level and column-level lineage tracking\",\n",
    "            \"Impact analysis for changes\",\n",
    "            \"Data classification and governance\",\n",
    "            \"Access audit and monitoring\"\n",
    "        ],\n",
    "        \"key_tables\": [\n",
    "            \"system.access.table_lineage\",\n",
    "            \"system.access.column_lineage\",\n",
    "            \"system.access.audit\"\n",
    "        ]\n",
    "    },\n",
    "    \"Delta Live Tables\": {\n",
    "        \"benefits\": [\n",
    "            \"Declarative pipeline development\",\n",
    "            \"Built-in data quality expectations\",\n",
    "            \"Automatic dependency resolution\",\n",
    "            \"Comprehensive monitoring and observability\"\n",
    "        ],\n",
    "        \"expectation_types\": [\n",
    "            \"@dlt.expect - warn on violation\",\n",
    "            \"@dlt.expect_or_drop - drop invalid rows\",\n",
    "            \"@dlt.expect_or_fail - fail pipeline on violation\"\n",
    "        ]\n",
    "    },\n",
    "    \"Databricks Jobs\": {\n",
    "        \"orchestration_features\": [\n",
    "            \"Multi-task workflows with dependencies\",\n",
    "            \"Flexible scheduling options\",\n",
    "            \"Error handling and retry policies\",\n",
    "            \"Comprehensive notifications\"\n",
    "        ],\n",
    "        \"monitoring_tables\": [\n",
    "            \"system.lakeflow.job_runs\",\n",
    "            \"system.lakeflow.pipeline_updates\",\n",
    "            \"system.lakeflow.pipeline_events\"\n",
    "        ]\n",
    "    },\n",
    "    \"Best Practices\": [\n",
    "        \"Use Unity Catalog for comprehensive governance\",\n",
    "        \"Implement proper data quality expectations in DLT\",\n",
    "        \"Design robust error handling and retry logic\",\n",
    "        \"Monitor pipeline health and performance regularly\",\n",
    "        \"Use environment-specific configurations\",\n",
    "        \"Implement proper Git workflows for deployment\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Week 6 Summary - Dataflows, Lineage, Orchestration & Automation\")\n",
    "print(\"=\" * 70)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Week 6 Completion Checklist\n",
    "\n",
    "- ‚úÖ Explored Unity Catalog lineage capabilities\n",
    "- ‚úÖ Created sample DLT pipeline with data quality expectations\n",
    "- ‚úÖ Designed multi-task job configurations\n",
    "- ‚úÖ Implemented monitoring and observability patterns\n",
    "- ‚úÖ Developed troubleshooting and debugging approaches\n",
    "- ‚úÖ Established best practices for production deployment\n",
    "\n",
    "**Key Skills Acquired:**\n",
    "- Data lineage analysis and impact assessment\n",
    "- Declarative pipeline development with DLT\n",
    "- Workflow orchestration and error handling\n",
    "- Production monitoring and alerting\n",
    "- Environment management and deployment automation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
